\documentclass[a4paper,10pt]{book}
\usepackage[utf8x]{inputenc}
\usepackage{graphicx}

\begin{document}

\author{The SonicLib group}
\title{SonicLib Reference Manual}
\maketitle

\newpage
\tableofcontents
\newpage

%*************************************************************************
\chapter{Introduction to SonicLib}
\label{chap:SonicLib}

%*************************************************************************
\section{What SonicLib is for}
\label{sec:SonicLib purpose}

The last years have seen a steady diffusion of three-dimensional ultrasonic anemometers, accompanied by a spread of applications from scientific research to environment and human health protection, siting and monitoring of wind power plants, and many more.

This is not surprising as instrument costs have greatly reduced while performance and robustness did improve. Compared to mechanical anemometers, ultrasonic anemometers enjoy many practical advantages. Among them, and with no claim of completeness:

\begin{itemize}
 \item Ultrasonic anemometer are immune to ``wind calms'': they can record extremely slow or irregular winds.
 \item Having no moving parts an ultrasonic anemometer minimizes low-pass filtering and resonance on input signal.
 \item Most ultrasonic anemometers have accuracy and resolution greatly exceeding mechanical models.
\end{itemize}

Last but not least, ultrasonic anemometers may sample wind very fast, at rates of many tens of Hz. This characteristic, together with an inherent high sensitivity, allow using ultrasonic anemometers to measure directly atmospheric turbulence.

All of this does not come free: respect to mechanical anemometers, sonic anemometers produce a huge mass of data. Gathering it, recording in an organized manner and extracting useful information demand non-trivial processing.

SonicLib attempts to satisfy this need in a simple to use way, yet allowing users to understand the details of what's going on, if they want/need.

\section{Why a new tool?}
\label{sec:SonicLib motivation}

Much work has been done in the past on sonic anemometer data, and many excellent tools have been developed as a result. Mentioning them all would be quite an endeavour, but sure the question of why trying developing a \emph{new} one makes some sense arises spontaneously.

These are our tentative, multiple answers:

\begin{itemize}
 \item We would like to really learn, and there is no better way of achieving this objective than doing on our own. This is after all how SonicLib started: as the visible outcome of the atmospheric turbulence laboratory held at Physics department in University of Milan on 2012.
 \item Of the many treatments involving sonic anemometer data final processing is perhaps the most delicate: Were our computations sensible? Are the physical assumptions they base upon really satisfied by current data set? Is accuracy known? These, and other similar questions of paramount importance, devise sensible answers, much easier to obtain if we users ``own'' the code.
 \item Much work has been made, but much is to be still done, too. This is true of both the techniques and the foundations. We strongly feel a diffuse need exists of an idea exploration platform, which is as easy to modify and extend as possible. This platform is not likely to take the form of a product-like software package.
 \item Most existing software packages focus narrowly on one or few applications (typically on flux measurement in the context of finite duration experiments). But the scope of possible uses of sonic anemometers is much broader than that, and we feel the moment is mature for flexibility, in addition to excellence.
\end{itemize}

These motivations have led us to conceive SonicLib, and helped us to define some basic requirements on it:

\begin{itemize}
 \item SonicLib should be useable both as a toolbox with which to compose processing procedures, and interactively as a sort of calculator. This is why we decided using an interpreted language, and not a compiler.
 \item All SonicLib source code should be freely available for inspection, study, extension and correction. That is, SonicLib should be at least \emph{open source}. And in addition, it should be \emph{easily understandable}, even if writing it this way might decrease its efficiency (but just note how even the tiniest netbooks in use to date may deliver massive, and almost always unused, computing power). Simple solutions are preferable, in our opinion, to complicate ones. Simple programs, by the ways, are easier to change, or throw away and replace, if they are wrong.
 \item As a corollary of the preceding point, we took the early decision of adopting a programming language which is as close as possible to the nature of the problems at stake. Our choice went to R, the publicly available implementation of the S statistical language. Its almost limitless support of computational statistics, backed by an immense and growing user basis, matches quite well the current ``statistical'' flavor of sonic anemometer data analysis and processing. Besides, R offers huge graphical and scientific computing power.
\end{itemize}

All that said, we understand being in debt with the research groups and individuals who preceded us. To them all goes our gratitude. Thank you all, and forgive our immense ignorance and immodesty.

We're sincerely and authentically in love with the subject, however.

%*************************************************************************
\section{Why R?}

We might have decide to develop SonicLib as a Fortran library, and maybe a number of small utility programs written once again in Fortran.

That would no doubt have allowed us to reach a great efficiency, and may have been a great occasion to show our programming prowess.

But this would have missed our most important concern: doing something easy to understand and use.

To date many incredible software packages exist of a very high level, like Python and SciPy, or Octave, or Matlab. Respect to Fortran they are more expressive, in the sense they require much less writing to reach the desired result.

R (see \cite{RManual}) is one of these high level languages. The reasons we adopted it for SonicLib are:

\begin{itemize}
 \item R finds its natural niche in the field of data analysis, and ultrasonic anemometer data processing is a special case of it.
 \item R provides extensive support for computational methods, statistics and graphics: all these play an important role in ultrasonic anemometer data processing.
 \item R is supported by the work of a very large comunity of volunteers, and is a healthy and evolving project.
 \item R is open source, and hence gives an opportunity to understand the innards without the refrain coming from the infringement of commercial secrets.
\end{itemize}

\section{Style matters}

Using R, a high level language, is a wonderful occasion, but possibly also a nightmare: expressivity may be used to write code which is shorter to read, easier to understand, less cumbersome to memorize (in our heads, not computer RAM banks), and lovely to understand. But also, by adopting the most terse and idiomatic language constructs would make code a real mess (possibly very efficient).

In SonicLib our top priority is being understood, and communicate. Efficiency is ``also'' important, but comes second: would a conflict arise, readability and understandability will prevail.

As you will see by inspecting SonicLib sources, code has been written ``the simple way'', sometimes despite speed of execution or even ``elegance''. This is entirely intentional: the coding equivalent of a curvy and a bit dull readable handwriting, opposed to those scrawls which show a lot of personality but do not convey useful information.

The possible loss of efficiency might be a problem, but we feel it is really not. Today's reasonable portable computers pack enough power to largely exceed the specifications of supercomputers some the authors have met in their student life in the Eighties of last century. Used the right way they are fast enough to let us forget any efficiency concern.

For the very same reasons we invested most of our time in this Manual: as humble as it is, it got no less than 70\% of total development time. Another advantage of the expressivity and conciseness of R: less code to write means having more time to spend communicating with people.

%*************************************************************************
\section{The SonicLib Development Process}
\label{sec:SonicLib Development Process}

At least initially we strive to deliver SonicLib as a single R source file, named \texttt{SonicLib.r}, and complemented by a reference guide (this document).

The official source and reference manual are distributed through the project site, \texttt{http://www.boundarylayer.eu}.

Developers, in addition to maintaining \texttt{SonicLib.r} and issuing new releases of it, also work on experimental code which is shared through e-mails among work group members, on a voluntary basis. After consensus has settled on experimental code, this is moved to the \texttt{SonicLib.r} file, and a new version is then released to the public.

This way of doing is maybe not the most secure, but as far as the work group dimension is ``small'' its simplicity outweights its possible drawbacks.

SonicLib is the result of co-creation among people who may act both as leaders or followers, as needed/possible/desired. After all nothing requires the figure of leader to be unique in an organization. As a community, we organized in a way some personnel management consultants would name a \emph{web of inclusion} (\cite{Helgesen1995}). Inside of it anyone of us has an opportunity to settle where their attitudes and experience are best fitting. And to change, if they will, as they evolve.

This fits well with the multi-faceted (and related) cooking pots we must stir almost simultaneously:

\begin{itemize}
 \item Learning the theory and techniques.
 \item Experimenting new ideas.
 \item Writing code.
 \item Performing tests.
 \item Writing documentation.
 \item Finding and organizing data sets.
 \item Relating with people inside and outside the work group.
 \item Building harmony and community within.
 \item Finding ways to present (to graph, film, describe) sonic anemometer data and processing results.
\end{itemize}

%*************************************************************************
\section{Symbol Glossary}
\label{sec:Glossary}

\begin{description}
 \item[$f_{s}$] Sampling rate (s).
 \item[$L$] Obukhov length.
 \item[$n$] Number of data actually present in a sonic raw data files.
 \item[$n_{s}$] Number of data expected in a (hourly) file, identical to $3600 \cdot f_{s}$.
 \item[$t_{a}$] Averaging time, defined as length of averaging period expressed in seconds.
 \item[$\alpha$] First rotation angle in eddy covariance, or individual rotation angle in planar fit.
 \item[$\beta$] Second rotation angle in eddy covariance, or first plane rotation angle in planar fit.
 \item[$\delta$] Third rotation angle in eddy covariance, or second plane rotation angle in planar fit.
\end{description}


%*************************************************************************
\chapter{SonicLib Workflow}
\label{chap:Workflow}

\section{Why a Workflow?}
\label{sec:Workflow concept}

Many different ways exist when performing data processing, and this is specially true in cases like the processing of ultrasonic anemometers, where steps are many and some of them might be interchanged.

All these methods may be broadly classified in two blocks:

\begin{itemize}
 \item Invoking a fixed sequence of steps within an automatic data processing program, or
 \item Performing the steps interactively, checking intermediate results.
\end{itemize}

Of these blocks the first is gaining diffusion along with large networks gathering data continuously in unattended mode, where it is imperative to process data as soon as they are available in sufficient number, not being realistic in most cases to simply transfer bulk raw data files to a data processing center.

Automatic sequences are fixed however, and their rigidity makes their use as an easy to follow use narrative of quite little interest.

Because of this, we'll imagine processing to occur interactively.

Interactive data processing is by definition flexible. But nonetheless, if perused will tend to crystallize along a more defined ``workflow''. \emph{This} can provide a better use narrative.

Of course many workflows are possible, as we've seen for chain of automatic data processing programs. Se we'll pick one at random, and use it as a metaphor of what could be accomplished interactively using SonicLib building blocks. The workflow adopted, as many other, may be used as a basis for a possible automatic procedure.

\section{Reference Workflow}
\label{sec:Reference workflow}

Our reference workflow is defined within the context of a measurement campaign, in which data from a single sampling point are retrieved from time to time.

The major steps of the reference workflow are:

\begin{itemize}
 \item Original data, maybe in binary form, are converted to a convenient ASCII form, namely SonicLib standard raw data file. As a first effect of this conversion, data may be visually inspected or plotted. This possibility is very powerful, as it allows to see whether data collection has generated or not some macroscopic error, and to perform the first open minded explorations.
 \item SonicLib raw files are sequentially read and ``averaged'', so that a set of mean, variance and covariance values is obtained. This step is supported in fact by a specific function in SonicLib, \verb|average.sonic.file.set|. Averaging may be viewed in various ways: as an artifice to filter a ``noisy'' signal, and as a way to ``extract'' useful information from the huge mass of raw data. The averaging step needs a choice, by the users, of an averaging step, whose choice may yield in some occasions quite different patterns.
 \item During averaging a first set of quality indicators, as number of elementary data used per average, relative non-stationarity of wind components, non-steadiness for covariances), have been generated. Now they may be used, in addition to some key averages and covariances, to locate macroscopic deviates and exceptions. This allows to form a first impression about overall quality of data, and locate specific SonicLib raw data files where to look for in greater depth. Depending on the objectives of the campaign, this may also be the point where a subset of ``good'' data is elected and put apart for further study. Or in alternative, when to flag some data as ``bad'' in view of their imputation )thet is, replaced with an estimate, as needs to be made for example when preparing inputs for Lagrangian dispersion models).
 \item Why exceptional data are so exceptional? In some cases budget restrictions do not encourage answering such a non immediately productive question: you may just remove them from the official set, after all. Yet reflecting on outliers may help fostering a deeper understanding of the data set. For example you may discover site-related idiosynchrasies (like that large beautiful oak on SW responsible of generating massive turbulence with her leaves), or a sonic anemometer showing the first sign of failure, or an almost-systematic trouble with power supply. SonicLib supports investigate on these ``whys'' by means of routines which can stitch consecutive hours together, extract specific blocks from an hour, plot actual values and spectra, investigate auto- and cross-correlation. These very same instruments may of course be also used to study less exceptional data, gaining a better understanding on the campaign.
 \item Averages may be interesting to an expert eye, but quite seldom are informative to the layperson, or the mindless rigidity of a dispersion model. To get some sense from them they should be translate to physically meaningful quantities like energy fluxes, turbulence and stability indicators, or precision wind vector. This is done by invoking one of the processing routines like eddy covariance (SonicLib supports both classical and planar fit flavors) or wind turbine specific calculations.
 \item Once physically meaningful data have been obtained (and possibly ``good'' data have been separated from junk) the residual problem exists of delivering them in a form suitable for use. This may be well organized data files. Or graphs, suitable for inclusion in a scientific paper or technical report. Data restitution is addressed by SonicLib in various functions, many of them allowing extensive customization.
\end{itemize}

\section{Workflow: A More Specific Example}
\label{sec:More specific workflow}

Maybe, a specific and hands-on example of workflow is easier to understand than a zillion words. So let's imagine we desire to use SonicLib to analyze a data set in view, say, of a scientific paper or a technical report.

First of all, we need to know our starting data. In which format are them? Almost surely, they are not written in SonicLib raw data format (described in section \ref{sec:FileFormat}), and rather are available in some other form.

The first question then is, whether a converter from this form to SonicLib raw format is available. If not so, then the first step is writing one (and, possibly, contributing it to the SonicLib group).

But suppose a converter already exists. Then, we first need using it - maybe after having read the appropriate section in chapter \ref{chap:Converters} of this Manual.

The actual mechanism will in general differ somewhat depending on input form (and then converter), but in most cases what will actually happen is that some program is run automatically on the original file set, to produce a collection of SonicLib files containing exactly the same information, just in a different format allowing SonicLib to access the data.

In this (indeed very delicate) step a bit of organization helps. As most converter procedures operate \emph{en masse} on all files in a directory, producing a SonicLib file set in another directory, it is highly advisable to place all original file of the same campaign in a single directory, or a seto of related directories, with no other data in them. And also, it makes sense to store converted data in another, separate and specific directory. This is not difficult, if an operating system like UNIX, Linux or Windows is used, with their extensive support for creating and managing directories.

To be specific, let's imagine our input files are in SLT form, with all data of the Second Capocaccia 2012 Campaign in directory \verb|/raw/Capocaccia/2012/Second|. We first decide the converted data will be in (say) directory \verb|/datasets/Capocaccia/2012/Second|, and then run procedure \verb|convertSTL.py| which enumerates all the hours within the input directory, launching on all of them the actual converter \verb|readslt|.

As you will discover in section \ref{sec:Convert SLT}, in order to run \verb|convertSTL.py| you need to collect all information about your data, especially how many columns have been gathered in addition to the sonic quadruples, and which unit conversion rule to apply on each of them. This information will result in a small configuration file, whose contents is read by the conversion procedure and used to generate the SonicLib raw data files.

If this little decision has been taken correctly, after running the program \verb|convertSTL| you will find in the destination directory as many SonicLib data files as the number of hours in original data.

The next step is might be to checking data quality in some simple way. But unfortunately the number of raw data may be immense, much larger than a human might cope with: at 10 Hz, a sonic anemometer produces 36000 data rows per hour, and a typical campaign may endure a couple weeks, that is around 14 by 24 hours, that is 336 hours, or more than twelve millions data records. What may realistically be checked at this point is plainly the file size, trying to locate hours during which an abnormal amount of data has been collected.

Luckily, SonicLib allows to combine this, and other more sophisticated, data quality checks with mass averaging: function \verb|average.sonic.file.set| reads sequentially, in the right time order, all SonicLib files in \verb|/datasets/Capocaccia/2012/Second|, and computes all averages on the time basis selected by user. While doing so, it also counts the number of data per hourly file, compares it with the minimum acceptable threshold, and discards automatically all non-conforming hours.

On the hours surviving this first check, determination of non-stationarity is also performed on key data column.

If \verb|average.sonic.file.set| terminates successfully, then memory contains a new data item holding all results.

As most intermediate results in SonicLib, the output from \verb|average.sonic.file.set| is an R list \emph{belonging to a class} (in this case, \verb|sonic.avg.data|). If you are a ``final'' user, then the result belonging to a very specific class means very little. But to SonicLib this attribute is of great help, as any step checks its input to be of the right type: if it is, then all rights and the desired computation is done; if it is not, processing stops immediately and, if an appropriate option has been specified, a nice message is printed to the R session informing on what happened.

But before to proceed with the second processing step, perhaps we like to check data quality. As explained before this is possible by checking averages directly (for example by plotting them and looking for anomalous values) or using non-stationarity indicators to lead the way. This step, of a very visual nature, greatly benefits of the extensive graphic capabilities of R. Some ultrasonic anemometer specific graphs (for example autocorrelation, cross-correlation, ogive) are implemented within SonicLib as extensions to R, in a way straightforward enough to allow easy customization would this be needed.

What to do, once data quality has been checked using averaged values? The answer depends of course on context - namely, on what we want to do with data. In our very specific case, writing a paper, we may be free to reject data not fulfilling our quality criteria (this may not happen in other cases, like for example using sonic data to feed a lagrangian pollutant dispersion model).

In order to steer rejection with minimum hassle, it may be convenient to assign each row in averaged set a \emph{score} weighting for example the ``number of troubles''. Then, you may use the powerful R indexing on condition capabilities to retain only data with a ``trouble load'' smaller than some predefined positive value.

Once you have decided which data to retain in your analysis, it comes finally the time to actually process them. In a common case, this means to perform eddy covariance over the averaged values, which in SonicLib is made using function \verb|eddy.covariance|. Other data processing functions are available in SonicLib, meeting the needs of other application domains (for example, wind energy). This broader than usual scope is one of the distinctive features of SonicLib, compared to other ultrasonic anemometer data processing libraries - but in this example we'll not pursue this way.

Rather we notice that function \verb|eddy.covariance| has taken as input a data item of class \verb|sonic.avg.data|, worked on it, enriched it with (many) additional columns, and made the result available as another data set, this time of time \verb|sonic.eddy.cov.data|.

Two questions may arise at this point.

First, we may wonder how to preserve our results: after all, all processing steps we've performed in R did produce data in memory. We saved nothing, at least apparently.

And then, maybe more importantly, we may ask ourselves what to do with data.

We'll attempt some possible and sensible answers in the next two subsections, from a purely computer-usage standpoint. Of course the questions are deeper, and involve a part dealing with scientific meaning, opportunity, and stuff like that. But in this example let us be plain and simple. We promise to deal with some more cogent subjects in later more operational sections, to the extent reasonable in a Manual.

\subsection{Preserving with results after we got them}

Fine! You just have completed your first eddy covariance processing, and placed it into an R variable \verb|ec|. Maybe this seems not that a great endeavor (after all the computer performed all the hard work for us), but sure it did cost some work.

More important: suppose to throw away the result \verb|ec|. If you really needed this data set the problem arises to rebuild it: how may you \emph{guarantee} you will obtain the very same result?

In theory the answer is trivial: if you give the functions exactly the same data and options then the computer will reconstruct your results with digital accuracy. The real question is: how may you be sure you will use the same parameters? This seems silly, when you imagine having to rebuild a data set after five minutes. But wait five months, maybe under referee pressure, and you'll agree with us.

This is an example of the classical data processing nightmare of (lack of) \emph{configuration management}.

To overcome this issue there exist many little tricks. We share a couple (if you already know them, please forgive our silliness).

A first way to be able to redo a computation exactly in the same way as before is by embedding it in an \emph{automatic procedure}. This approach works, as far as we have seen, using R in general, and not only with SonicLib. Additionally: if instead of using the bare R terminal session you resort to an IDE (like for example RStudio) then it is very easy to perform computations interactively, copy them line by line, and pasting to a script file which can be executed in future at will. Giving this script a name easy to remember or distinguish helps somewhat; one of us for example packs all processing command for a paper in a script named \verb|process.r|. Using the same name for any different campaign allows to get at the processing script with mindless certainty, something helpful under stress.

A second way is, saving results permanently. R allows to save data in many ways. Maybe the simple is saving the session when you leave R, by answering a \verb|y| after command \verb|q()|, when R asks whether you want to save the current session.

As easy as this approach is, it also has a major drawback: the more you work with an interactive session, save it, then restore on the next day, the larger the set of defined variables becomes. R saves all these data in memory, and if you pay no attention you may end up with many tens of megabytes of physical memory allocated to data you whose meaning no one may remember.

A cleaner approach is saving data items by hand, selectively. In SonicLib, because of the way results are held in memory (that is: as lists, one of whose element is the data frame containing the ``real data''), you may want to save the whole result variable, or the data frame only.

In the first case you may use the \verb|save()| command. If you check R help you will discover \verb|save()| takes some parameters. One of them is the name of the file you desire your data be saved to, and another is the variable you want to save. Data stored to disk using \verb|save()| may be read back into memory using the sister command \verb|load()|.

In the second case you will want to use command \verb|write.csv()|, with the data frame only as the variable to save. Command \verb|write.csv()| creates a CSV file you may read later on in a spreadsheet or a database program, extending your data analysis capabilities. The CSV file may be read back to memory using command \verb|read.csv()|.

\subsection{What to do with data, once we got them}

No question, averaging and eddy covariance produce \emph{many} columns of data. These columns are stored to member \verb|data| in the variable helding results in memory as a \emph{data frame}, a powerful R tabular data structure you may use in very many ways.

The simplest thing you may do is retrieving columns as they are:

\begin{verbatim}
u <- ec$data$u.avg
v <- ec$data$v.avg
w <- ec$data$w.avg
t <- ec$data$t.avg
time.stamp <- ec$data$t.stamp
\end{verbatim}

\noindent You may then plot your data:

\begin{verbatim}
plot(time.stamp, sqrt(u^2+v^2),type="l")
\end{verbatim}

\noindent Or, perform any kind of statistical analysis:

\begin{verbatim}
median(sqrt(u^2+v^2))
\end{verbatim}

In short, once you have produced your data you have all the power of R to analyze, transform, read/write them. Only your creativity will be a limit.

\section{Automatic Procedures: Usefulness and Danger}
\label{sec:Automatic procedures}

Measurement campaign of finite duration are just one of the possible uses of sonic anemometers. Another use, quite common on those days, is as part of measurement stations operating in continuous mode within a station network.

In this case, many station produce simultaneous streams of data continuously, in conditions often completely out the reach of an experimenter's control. The processing of this immensely huge flow of data can not be realistically made by a single (or multiple, for all what matters) human operator. By necessity we have to rely on \emph{automatic procedures}.

Usefulness of automatic procedures is then obvious: they allow making some sense out of an otherwise unmanageable pile of meaningless numbers, at a sustainable cost.

But automatic procedures bear with them a sort of original flaw: their operation being fixed and predictable, they tend to induce laypeople perceiving them as invincible oracles. Real world often lacks this high level of reliability however, and sonic anemometer data processing is no exception.

Maybe, the most striking difference between an interactive workflow (like the one we adopted as reference) and an automatic procedure is the former allows a greater opportunity of \emph{performing quality checks} on data and intermediate results. This can not be done at the same extensive level by an automatic procedure, which by its very nature is rigid.

By their very nature, wind measurements are highly site-dependent. Not only are sites different one from another in a myriad of details affecting for example rejection of outliers: they also change their characteristics with time. The data processing should reflect these evolving individualities: what's normal at a site might well be rubbish at another.

Just as a practical example, we may consider a measurement site intentionally placed within a plant canopy. There is perfectly normal that friction velocity exceeds the 30\% of wind speed. An intelligent automatic procedure should be able to ascertain the ``normality'' of situation, and to \emph{not} removed high $u_{*}$ records from the ``good data'' set.

But what's here true of friction velocity may be true in another place for wind vertical speed from a direction, and at still another place for a strange negative peak in sensible heat flux.

The possibilities are so numerous (in the order of billions) that trying to devise an adaptive and sensible response to all of them would result in a monster of complexity. And since procedure complexity is inversely poportional to reliability, such a behemoth would almost surely be very smart in theory, but unusable in practice.

Maybe, a more realistic approach consists in designing automatic procedures which are flexible enough to allow treating the most common key cases by a few \emph{configuration options}, and use it in the majority of ``quiet'' sites. Then, at sites where things are really awkward, the possibility exists of using ad-hoc procedures (also aautomatical).

The most appropriate value of configuration options then could be identified by means of \emph{characterization campaigns}, one per site, aimed at understanding what specifically happens there to wind. Or, if you prefer, what ``normal'' means there.

Configuration options also bear an original sin with them: once established, users may tend to figure them out as fixed and immutable. Site characteristics change with time however, and as part of a sensible maintenance protocol configuration should be routinely adapted to the new circumstances. How? Of course, with new characterization campaigns.

Now: characterizaton campaigns are done on raw, not pre-processed data. For characterization to be done and re-done without stopping station operation for months it is necessary to store raw data at any station for a long time (at least one year). This is made routinely and transparently by data loggers designed specifically for sonic anemometers (like for example Meteoflux Core, by Servizi Territorio srl), but might not be present a priori.

And once you have downloaded your bunch of characterization data, you still have to use an interactive workflow to explore data, define normality on solid statistical grounds, and devise revised values for configuration parameters.

We may agree, then, that automatic procedures do not really operate unattended, abandoned to themselves, relying on the ultrasonic anemometer inherent reliability to produce a stream of data more and more garbled as conditions change from when configuration parameters have originally been devised. Rather, in a healthy processing environment procedures operate as faithful robots the way human operators would, would they have time to do, with someone governing them by adjusting this knob here, moving that switch there, and checking all runs smoothly.

Then, in practice, automatic procedures and interactive workflow can not be blindly separated. Execution of the latter forms the basis of making the former tick.

%*************************************************************************
\chapter{Data Types}
\label{chap:DataTypes}

This chapter contains the description of the most important data types used within SonicLib, as for example the standard file format for ultrasonic anemometer raw data. Other, less important data structures are described as part of individual function descriptions.

\section{Standard Ultrasonic Anemometer Raw Data Files}
\label{sec:FileFormat}

Raw ultrasonic anemometer data are used within SonicLib mostly in the form of standard formatted CSV archives conforming to the naming specification

\begin{verbatim}
  YYYYMMDD.HH.csv
\end{verbatim}

\noindent where

\begin{description}
 \item[YYYY] represents the year;
 \item[MM] represents the month in year (01 to 12);
 \item[DD] is day in month (01 to 31);
 \item[HH] designates the hour (00 to 23).
\end{description}

Collectively the fields YYYY, MM, DD and HH identify a \emph{hourly time stamp} which refers to the beginning of data acquisition period for given hour. More specifically, the file named \texttt{20120501.12.csv} contains data from 2012-05-01 12:00:00 to 2012-05-01 13:00:00, the first time limit included, the second excluded.

Hourly time stamp used in file names is always referred to \emph{local} time. Different anemometers and campaigns are distinguished attributing each of them a different directory.

Files in standard raw format contain five mandatory columns containing within-hour time stamp and ultrasonic anemometer quadruples, and zero to any optional columns containing analog values.

Columns are comma-separated, with dot used as decimal separator.

\subsection{Mandatory Columns}
\label{sec:MandatoryColumns}

Mandatory columns are five, with the following names and meanings:
\begin{description}
 \item[time.stamp] Within-hour time stamp $0 \le t < 3600$, in seconds from beginning of hour; this data item should be accurate up to seconds, although fractional values are also in common use.
 \item[u] Wind component aligned to the West-East axis in local geographic reference (m/s); 
 \item[v] Wind component aligned to the South-North axis in local geographic reference (m/s); 
 \item[w] Wind component aligned to the Downwards-Upwards axis in local geographic reference (m/s); 
 \item[t] Ultrasonic anemometer temperature (Celsius degrees).
\end{description}

\subsection{Optional Columns}
\label{sec:OptionalColumns}

Optional columns may not exist in a file set. If on the contrary they do, then their names must belong the following list:

\begin{description}
 \item[q] Molar concentration of water ($\mathtt{mmol}/\mathtt{m}^{3}$); 
 \item[c] Molar concentration of carbon dioxide ($\mathtt{mmol}/\mathtt{m}^{3}$); 
 \item[a] Molar concentration of ammonia ($\mathtt{mmol}/\mathtt{m}^{3}$); 
 \item[m] Molar concentration of methane ($\mathtt{mmol}/\mathtt{m}^{3}$); 
 \item[temp] Temperature (Celsius degrees); 
 \item[hrel] Relative humidity (Celsius degrees). 
\end{description}

Two optional columns with same name are allowed to occur.

\subsection{Time stamps in SonicLib raw data files}
\label{sec:SonicLib Raw File Time Stamps}

The time stamp associated to each data record in a SonicLib data file is a very important data element, as it allows locating each sampled data in time.

Ideally, time stamps should be real numbers able to identify exactly the time instant a record has been produced. Data being collected sequentially in time, the sequence of time stamps should be monotonically increasing with respect to index value. In other words, if $t_{i}$ and $t_{i+1}$ denote the time stamps of two consecutive records, it should always be $t_{i} < t_{i+1}$.

Experience suggests however that real world time stamp are neither non-unique nor monotonically increasing.

About uniqueness: data collection equipment encode time stamps with a finite resolution. The reasons for this ranges from the finite resolution of existing (and future) real time clock chips found in computer systems, in the need of saving space on disk, and in many other reasons connected with engineering constraints and necessity of efficiency. That said, we can assume real world time stamps to be multiples of a fixed minimum time duration $\delta t$, whose length may be as long as 10 seconds in certain systems. Real world time stamps may then contain many repetitions, if time resolution $\delta t$ is larger than $1/f_{s}$. For example, a sonic anemometer operated at 10 Hz will produce data records whose time stamps, recorded with a time resolution of 1 s, will repeat 10 times. We then have to admit the possibility that $t_{i} \le t_{i+1}$.

This violation of uniqueness is not the only case of violation of increasing monotonicity. Real time clock circuits are in fact subject to drift, and sooner or later the time comes when their time must be adjusted. Depending on the direction of this adjustment, a gap or a wind-up in the sequence of time stamps may occur.

These possibilities lead us to introduce the concept of \emph{time stamp sequential regularity} (sorry for this ugly name - maybe we'll find a better one).

A sequence of time stamps $\{ t{i} \}$ is regular if the following conditions are met:

\begin{enumerate}
 \item For any indexes $i$ and $j$, with $i < j$, it is $t_{i} \le t_{j}$.
 \item In addition, $(j-i-1) \cdot \delta t \le t_{j} - t_{i} \le (j-i) \cdot \delta t$.
\end{enumerate}

A regular time stamp sequence is guaranteed to not contain winding or overlaps up to $\delta t$, which is sufficient in all practical applications.

Unfortunately, verifying directly that a sequence of time stamps is regular involves a number of operations in the order of $n^{2}$, where $n$ denotes the length of the series. This is less than acceptable for daily use, as it is not uncommon to deal with time stamp sequences longer than 36000 samples.

Given the importance of time stamp series regularity this sounds a pity, and may lead to search for some more or less clever algorithm characterized by a linear complexity. This path is quite interesting, but we will not follow it.

In fact, it may be observed that in a healthy data acquisition system events leading to time series irregularity occur very rarely, the two most common being real time clocks adjustment, and gaps induced by failures in the sonic anemometer, the data acquisition system, or the cable joining the two. (Less common cases are also possible, like sonic data sampling rate reprogramming).

But when such events do not happen (and this is the majority of cases) it always happens that a data file will contain a number of data, $N$, closely approximating the value $3600 \cdot f_{s}$ for hourly-organized files (like the ones described in this section). In these normal cases, the sequence of time stamps is also regular, and it makes a lot of practical sense to use the predicate $\left| N - 3600 \cdot f_{s} \right| < \varepsilon$ as a proxy of regularity. Checking the size of a data file is in fact a trivial thing, which may be accomplished in a single step (in R all you have to do is issuing a command like \verb|N <- length(d$data$time.stamp)|.

We notice \emph{en passant} that this criterion is not absolutely exact. It is not difficult to devise cases in which $\left| N - 3600 \cdot f_{s} \right| < \varepsilon$ but the series is irregular. But these cases arise from the accidental combination of unfrequent cases like real time clock adjustment in backward direction plus a failure in the sonic suppressing emission of a number of data exactly equal to the product of sonic sampling rate and real time clock adjustment. The probability associated to these independent rare event is the product of their component probabilities, a number so small that we can safely neglect these combined events.

The problem then arises to identify sensible values for the tolerance $\varepsilon$. A reasonable tolerance should include the little clock divergences naturally occurring with existing technology.

An example of these irregularities is the actual data acquisition frequency may deviate slightly from the nominal. The practical reason this can (and does) happen is that event generation used to trigger sonic anemometer readings is often delegated to counters, as found in many microcontrollers. These counters generate pulses according to a time base established by the processor quartz, and this latter is not necessarily an integer sub-multiple of one second.

These deviations are usually small, but may reach a 0.02 \%. Deviations like these are practically ininfluent on ultrasonic anemometer data processing, but must be taken into account when setting a sensible tolerance. 

``Reasonable'' default thresholds for tolerance $\varepsilon$ are established, and documented, for routines dealing with data input. Their value may be altered to reflect more detailed knowledge. Upon reaching or exceeding the threshold data read functions reject the current hour, thus allowing all other SonicLib components to assume time stamps are regular.

%*************************************************************************
\section{The object type ``sonic.raw.data''}
\label{sec:sonic.raw.data}

Objects of type \verb|sonic.raw.data| are created when reading raw data files, and are lists composed by two members:

\begin{description}
 \item[data] Data frame, containing columns with same names as the ones found in standard ultrasonic anemometer raw data files (see \ref{sec:FileFormat} for details).
 \item[sampling.rate] Nominal sampling rate, specified as an argument of function get.raw.data (see \ref{sec:Get Raw Data File}).
\end{description}


%*************************************************************************
\section{The object type ``sonic.avg.data''}
\label{sec:sonic.avg.data}

Objects of type \verb|sonic.avg.data| are created using function \verb|average.sonic.data|. They are lists defined as follows:

\begin{description}
 \item[data] Data frame, containing averages and stationarity indicators as described later on.
 \item[delay] Value of parameter \verb|delay| as passed in call, for future reference.
 \item[sampling.rate] Nominal sampling rate, as coming from the data object \verb|sonic.raw.data| from which averages have been computed (see \ref{sec:Average Single Raw Data File}).
 \item[averaging.time] Averaging time used when processing data.
\end{description}

The \verb|data| element in objects of \verb|sonic.avg.data| class, of type ``data frame'', is composed by the following columns:

\begin{description}
 \item[t.stamp] Time stamp identifying the beginning of averaging interval.
 \item[n.data] Number of data in averaging interval (should be indicatively equal to $f_{s} \cdot t_{a}$, the averaging time. It may differ a little bit, due to clock imprecision in sonic or data acquisition system (or both). If much smaller than expected value, then it is legitimate to suspect a gap in data (most often due to power failure, data acquisition system failure, or ultrasonic anemometer lock).
 \item[u.avg] Mean value of wind component $u$, in m/s, computed on valid data.
 \item[v.avg] Mean value of wind component $v$, in m/s, computed on valid data.
 \item[w.avg] Mean value of wind component $w$, in m/s, computed on valid data.
 \item[t.avg] Mean value of sonic temperature, in Celsius degrees, computed on valid data.
 \item[uu] Covariance between wind components $u$ and $u$ (that is, variance of $u$) in $\mbox{m}^2/\mbox{s}^2$.
 \item[uv] Covariance between wind components $u$ and $v$ in $\mbox{m}^2/\mbox{s}^2$.
 \item[uw] Covariance between wind components $u$ and $w$ in $\mbox{m}^2/\mbox{s}^2$.
 \item[vv] Covariance between wind components $v$ and $v$ (that is, variance of $v$) in $\mbox{m}^2/\mbox{s}^2$.
 \item[vw] Covariance between wind components $v$ and $w$ in $\mbox{m}^2/\mbox{s}^2$.
 \item[ww] Covariance between wind components $w$ and $w$ (that is, variance of $w$) in $\mbox{m}^2/\mbox{s}^2$.
 \item[ut] Covariance between wind components $u$ and sonic temperature in $\mbox{m C}/\mbox{s}^2$.
 \item[vt] Covariance between wind components $v$ and sonic temperature in $\mbox{m C}/\mbox{s}^2$.
 \item[wt] Covariance between wind components $w$ and sonic temperature in $\mbox{m C}/\mbox{s}^2$.
 \item[vel3] Mean of wind speed elementary data in the instrument's horizontal plane, in $\mbox{m}^{3}/\mbox{s}^{3}$.
 \item[vel3.3] Mean of wind speed elementary data using all three wind vector components, in $\mbox{m}^{3}/\mbox{s}^{3}$.
 \item[u.rns] Relative non-stationarity of wind component $u$.
 \item[v.rns] Relative non-stationarity of wind component $v$.
 \item[w.rns] Relative non-stationarity of wind component $w$.
 \item[uu.nst] Non-steadiness of covariance $\overline{u'u'}$.
 \item[uv.nst] Non-steadiness of covariance $\overline{u'v'}$.
 \item[uw.nst] Non-steadiness of covariance $\overline{u'w'}$.
 \item[vv.nst] Non-steadiness of covariance $\overline{v'v'}$.
 \item[vw.nst] Non-steadiness of covariance $\overline{v'w'}$.
 \item[ww.nst] Non-steadiness of covariance $\overline{w'w'}$.
 \item[ut.nst] Non-steadiness of covariance $\overline{u't'}$.
 \item[vt.nst] Non-steadiness of covariance $\overline{v't'}$.
 \item[wt.nst] Non-steadiness of covariance $\overline{w't'}$.
 \item[q.avg] Mean molar concentration of water in $\mbox{mmol}/\mbox{m}^{3}$ (present only if water concentration is in corresponding raw data file).
 \item[uq] Covariance $\overline{u'q'}$ between $u$ wind component and molar concentration of water in $\mbox{mmol}/\mbox{m}^{3}$ (present only if water concentration is in corresponding raw data file).
 \item[vq] Covariance $\overline{v'q'}$ between $v$ wind component and molar concentration of water in $\mbox{mmol}/\mbox{m}^{3}$ (present only if water concentration is in corresponding raw data file).
 \item[wq] Covariance $\overline{w'q'}$ between $w$ wind component and molar concentration of water in $\mbox{mmol}/\mbox{m}^{3}$ (present only if water concentration is in corresponding raw data file).
 \item[uq.nst] Non-steadiness of covariance $\overline{u'q'}$ (present only if water concentration is in corresponding raw data file).
 \item[vq.nst] Non-steadiness of covariance $\overline{v'q'}$ (present only if water concentration is in corresponding raw data file).
 \item[wq.nst] Non-steadiness of covariance $\overline{w'q'}$ (present only if water concentration is in corresponding raw data file).
 \item[c.avg] Mean molar concentration of carbon dioxide in $\mbox{mmol}/\mbox{m}^{3}$ (present only if carbon dioxide concentration is in corresponding raw data file).
 \item[uc] Covariance $\overline{u'c'}$ between $u$ wind component and molar concentration of carbon dioxide in $\mbox{mmol}/\mbox{m}^{3}$ (present only if carbon dioxide concentration is in corresponding raw data file).
 \item[vc] Covariance $\overline{v'c'}$ between $v$ wind component and molar concentration of carbon dioxide in $\mbox{mmol}/\mbox{m}^{3}$ (present only if carbon dioxide concentration is in corresponding raw data file).
 \item[wc] Covariance $\overline{w'c'}$ between $w$ wind component and molar concentration of carbon dioxide in $\mbox{mmol}/\mbox{m}^{3}$ (present only if carbon dioxide concentration is in corresponding raw data file).
 \item[uc.nst] Non-steadiness of covariance $\overline{w'c'}$ (present only if carbon dioxide concentration is in corresponding raw data file).
 \item[vc.nst] Non-steadiness of covariance $\overline{v'c'}$ (present only if carbon dioxide concentration is in corresponding raw data file).
 \item[wc.nst] Non-steadiness of covariance $\overline{w'c'}$ (present only if carbon dioxide concentration is in corresponding raw data file).
 \item[a.avg] Mean molar concentration of ammonia in $\mbox{mmol}/\mbox{m}^{3}$ (present only if ammonia concentration is in corresponding raw data file).
 \item[ua] Covariance $\overline{u'a'}$ between $u$ wind component and molar concentration of ammonia in $\mbox{mmol}/\mbox{m}^{3}$ (present only if ammonia concentration is in corresponding raw data file).
 \item[va] Covariance $\overline{v'a'}$ between $v$ wind component and molar concentration of ammonia in $\mbox{mmol}/\mbox{m}^{3}$ (present only if ammonia concentration is in corresponding raw data file).
 \item[wa] Covariance $\overline{w'a'}$ between $w$ wind component and molar concentration of ammonia in $\mbox{mmol}/\mbox{m}^{3}$ (present only if ammonia concentration is in corresponding raw data file).
 \item[ua.nst] Non-steadiness of covariance $\overline{u'a'}$ (present only if ammonia concentration is in corresponding raw data file).
 \item[va.nst] Non-steadiness of covariance $\overline{v'a'}$ (present only if ammonia concentration is in corresponding raw data file).
 \item[wa.nst] Non-steadiness of covariance $\overline{w'a'}$ (present only if ammonia concentration is in corresponding raw data file).
 \item[m.avg] Mean molar concentration of methane in $\mbox{mmol}/\mbox{m}^{3}$ (present only if methane concentration is in corresponding raw data file).
 \item[um] Covariance $\overline{u'm'}$ between $u$ wind component and molar concentration of methane in $\mbox{mmol}/\mbox{m}^{3}$ (present only if methane concentration is in corresponding raw data file).
 \item[vm] Covariance $\overline{v'm'}$ between $v$ wind component and molar concentration of methane in $\mbox{mmol}/\mbox{m}^{3}$ (present only if methane concentration is in corresponding raw data file).
 \item[wm] Covariance $\overline{w'm'}$ between $w$ wind component and molar concentration of methane in $\mbox{mmol}/\mbox{m}^{3}$ (present only if methane concentration is in corresponding raw data file).
 \item[um.nst] Non-steadiness of covariance $\overline{u'm'}$ (present only if methane concentration is in corresponding raw data file).
 \item[vm.nst] Non-steadiness of covariance $\overline{v'm'}$ (present only if methane concentration is in corresponding raw data file).
 \item[wm.nst] Non-steadiness of covariance $\overline{w'm'}$ (present only if methane concentration is in corresponding raw data file).
 \item[temp.avg] Mean temperature in Celsius degrees (present only if thermometer data is present in corresponding raw data file).
 \item[hrel.avg] Mean relative humidity in \% (present only if hygrometer data is present in corresponding raw data file).
 \item[u.trend.slope] Slope of linear trend of wind component $u$ (present only if \verb|trend.removal| is ``linear'').
 \item[u.trend.constant] Intercept of linear trend of wind component $u$ (present only if \verb|trend.removal| is ``linear'').
 \item[v.trend.slope] Slope of linear trend of wind component $v$ (present only if \verb|trend.removal| is ``linear'').
 \item[v.trend.constant] Intercept of linear trend of wind component $v$ (present only if \verb|trend.removal| is ``linear'').
 \item[w.trend.slope] Slope of linear trend of wind component $w$ (present only if \verb|trend.removal| is ``linear'').
 \item[w.trend.constant] Intercept of linear trend of wind component $w$ (present only if \verb|trend.removal| is ``linear'').
 \item[t.trend.slope] Slope of linear trend of sonic temperature (present only if \verb|trend.removal| is ``linear'').
 \item[t.trend.constant] Slope of linear trend of sonic temperature (present only if \verb|trend.removal| is ``linear'').
 \item[q.trend.slope] Slope of linear trend of water vapor (present only if \verb|trend.removal| is ``linear'' and water is present).
 \item[q.trend.constant] Slope of linear trend of water vapor (present only if \verb|trend.removal| is ``linear'' and water is present).
 \item[c.trend.slope] Slope of linear trend of carbon dioxide (present only if \verb|trend.removal| is ``linear'' and carbon dioxide is present).
 \item[c.trend.constant] Slope of linear trend of carbon dioxide (present only if \verb|trend.removal| is ``linear'' and carbon dioxide is present).
 \item[a.trend.slope] Slope of linear trend of ammonia (present only if \verb|trend.removal| is ``linear'' and ammonia is present).
 \item[a.trend.constant] Slope of linear trend of ammonia (present only if \verb|trend.removal| is ``linear'' and ammonia is present).
 \item[m.trend.slope] Slope of linear trend of methane (present only if \verb|trend.removal| is ``linear'' and methane is present).
 \item[m.trend.constant] Slope of linear trend of methane (present only if \verb|trend.removal| is ``linear'' and methane is present).
 \item[temp.trend.slope] Slope of linear trend of temperature (present only if \verb|trend.removal| is ``linear'' and temperature is present).
 \item[temp.trend.constant] Slope of linear trend of temperature (present only if \verb|trend.removal| is ``linear'' and temperature is present).
 \item[hrel.trend.slope] Slope of linear trend of relative humidity (present only if \verb|trend.removal| is ``linear'' and relative humidity is present).
 \item[hrel.trend.constant] Slope of linear trend of relative humidity (present only if \verb|trend.removal| is ``linear'' and relative humidity is present).
\end{description}


%*************************************************************************
\section{The object type ``sonic.eddy.cov.data''}
\label{sec:sonic.eddy.cov.data}

Objects of type \verb|sonic.eddy.cov.data| are created upon application of eddy covariance or planar fit. A \verb|sonic.eddy.cov.data| object is a list, containing the following items:

\begin{description}
 \item[data] Data frame, containing results of eddy covariance processing.
 \item[delay] Value of parameter \verb|delay| as passed in call, for future reference.
 \item[sampling.rate] Nominal sampling rate, as coming from the data object \verb|sonic.raw.data| from which averages have been computed (see \ref{sec:Average Single Raw Data File}).
 \item[averaging.time] Averaging time used when processing data.
 \item[station.altitude] Height above mean sea level assumed for station (m).
 \item[processing] A string, whose value may be ``eddy.covariance'', ``eddy.covariance.3'' or ``planar.fit''.
\end{description}

The list item ``data'' is a superset of the homologous element found in \verb|sonic.avg.data|. The additional columns contain information specifically constructed during eddy covariance processing. The columns in ``data'' are:

\begin{description}
 \item[t.stamp] Time stamp identifying the beginning of averaging interval.
 \item[n.data] Number of data in averaging interval (should be indicatively equal to $f_{s} \cdot t_{a}$, the averaging time. It may differ a little bit, due to clock imprecision in sonic or data acquisition system (or both). If much smaller than expected value, then it is legitimate to suspect a gap in data (most often due to power failure, data acquisition system failure, or ultrasonic anemometer lock).
 \item[u.avg] Mean value of wind component $u$, in m/s, computed on valid data.
 \item[v.avg] Mean value of wind component $v$, in m/s, computed on valid data.
 \item[w.avg] Mean value of wind component $w$, in m/s, computed on valid data.
 \item[t.avg] Mean value of sonic temperature, in Celsius degrees, computed on valid data.
 \item[uu] Covariance between wind components $u$ and $u$ (that is, variance of $u$) in $\mbox{m}^2/\mbox{s}^2$.
 \item[uv] Covariance between wind components $u$ and $v$ in $\mbox{m}^2/\mbox{s}^2$.
 \item[uw] Covariance between wind components $u$ and $w$ in $\mbox{m}^2/\mbox{s}^2$.
 \item[vv] Covariance between wind components $v$ and $v$ (that is, variance of $v$) in $\mbox{m}^2/\mbox{s}^2$.
 \item[vw] Covariance between wind components $v$ and $w$ in $\mbox{m}^2/\mbox{s}^2$.
 \item[ww] Covariance between wind components $w$ and $w$ (that is, variance of $w$) in $\mbox{m}^2/\mbox{s}^2$.
 \item[ut] Covariance between wind components $u$ and sonic temperature in $\mbox{m C}/\mbox{s}^2$.
 \item[vt] Covariance between wind components $v$ and sonic temperature in $\mbox{m C}/\mbox{s}^2$.
 \item[wt] Covariance between wind components $w$ and sonic temperature in $\mbox{m C}/\mbox{s}^2$.
 \item[vel3] Mean of wind speed elementary data in the instrument's horizontal plane, in $\mbox{m}^{3}/\mbox{s}^{3}$.
 \item[vel3.3] Mean of wind speed elementary data using all three wind vector components, in $\mbox{m}^{3}/\mbox{s}^{3}$.
 \item[u.rns] Relative non-stationarity of wind component $u$.
 \item[v.rns] Relative non-stationarity of wind component $v$.
 \item[w.rns] Relative non-stationarity of wind component $w$.
 \item[uu.nst] Non-steadiness of covariance $\overline{u'u'}$.
 \item[uv.nst] Non-steadiness of covariance $\overline{u'v'}$.
 \item[uw.nst] Non-steadiness of covariance $\overline{u'w'}$.
 \item[vv.nst] Non-steadiness of covariance $\overline{v'v'}$.
 \item[vw.nst] Non-steadiness of covariance $\overline{v'w'}$.
 \item[ww.nst] Non-steadiness of covariance $\overline{w'w'}$.
 \item[ut.nst] Non-steadiness of covariance $\overline{u't'}$.
 \item[vt.nst] Non-steadiness of covariance $\overline{v't'}$.
 \item[wt.nst] Non-steadiness of covariance $\overline{w't'}$.
 \item[q.avg] Mean molar concentration of water in $\mbox{mmol}/\mbox{m}^{3}$ (present only if water concentration is in corresponding raw data file).
 \item[uq] Covariance $\overline{u'q'}$ between $u$ wind component and molar concentration of water in $\mbox{mmol}/\mbox{m}^{3}$ (present only if water concentration is in corresponding raw data file).
 \item[vq] Covariance $\overline{v'q'}$ between $v$ wind component and molar concentration of water in $\mbox{mmol}/\mbox{m}^{3}$ (present only if water concentration is in corresponding raw data file).
 \item[wq] Covariance $\overline{w'q'}$ between $w$ wind component and molar concentration of water in $\mbox{mmol}/\mbox{m}^{3}$ (present only if water concentration is in corresponding raw data file).
 \item[uq.nst] Non-steadiness of covariance $\overline{u'q'}$ (present only if water concentration is in corresponding raw data file).
 \item[vq.nst] Non-steadiness of covariance $\overline{v'q'}$ (present only if water concentration is in corresponding raw data file).
 \item[wq.nst] Non-steadiness of covariance $\overline{w'q'}$ (present only if water concentration is in corresponding raw data file).
 \item[c.avg] Mean molar concentration of carbon dioxide in $\mbox{mmol}/\mbox{m}^{3}$ (present only if carbon dioxide concentration is in corresponding raw data file).
 \item[uc] Covariance $\overline{u'c'}$ between $u$ wind component and molar concentration of carbon dioxide in $\mbox{mmol}/\mbox{m}^{3}$ (present only if carbon dioxide concentration is in corresponding raw data file).
 \item[vc] Covariance $\overline{v'c'}$ between $v$ wind component and molar concentration of carbon dioxide in $\mbox{mmol}/\mbox{m}^{3}$ (present only if carbon dioxide concentration is in corresponding raw data file).
 \item[wc] Covariance $\overline{w'c'}$ between $w$ wind component and molar concentration of carbon dioxide in $\mbox{mmol}/\mbox{m}^{3}$ (present only if carbon dioxide concentration is in corresponding raw data file).
 \item[uc.nst] Non-steadiness of covariance $\overline{w'c'}$ (present only if carbon dioxide concentration is in corresponding raw data file).
 \item[vc.nst] Non-steadiness of covariance $\overline{v'c'}$ (present only if carbon dioxide concentration is in corresponding raw data file).
 \item[wc.nst] Non-steadiness of covariance $\overline{w'c'}$ (present only if carbon dioxide concentration is in corresponding raw data file).
 \item[a.avg] Mean molar concentration of ammonia in $\mbox{mmol}/\mbox{m}^{3}$ (present only if ammonia concentration is in corresponding raw data file).
 \item[ua] Covariance $\overline{u'a'}$ between $u$ wind component and molar concentration of ammonia in $\mbox{mmol}/\mbox{m}^{3}$ (present only if ammonia concentration is in corresponding raw data file).
 \item[va] Covariance $\overline{v'a'}$ between $v$ wind component and molar concentration of ammonia in $\mbox{mmol}/\mbox{m}^{3}$ (present only if ammonia concentration is in corresponding raw data file).
 \item[wa] Covariance $\overline{w'a'}$ between $w$ wind component and molar concentration of ammonia in $\mbox{mmol}/\mbox{m}^{3}$ (present only if ammonia concentration is in corresponding raw data file).
 \item[ua.nst] Non-steadiness of covariance $\overline{u'a'}$ (present only if ammonia concentration is in corresponding raw data file).
 \item[va.nst] Non-steadiness of covariance $\overline{v'a'}$ (present only if ammonia concentration is in corresponding raw data file).
 \item[wa.nst] Non-steadiness of covariance $\overline{w'a'}$ (present only if ammonia concentration is in corresponding raw data file).
 \item[m.avg] Mean molar concentration of methane in $\mbox{mmol}/\mbox{m}^{3}$ (present only if methane concentration is in corresponding raw data file).
 \item[um] Covariance $\overline{u'm'}$ between $u$ wind component and molar concentration of methane in $\mbox{mmol}/\mbox{m}^{3}$ (present only if methane concentration is in corresponding raw data file).
 \item[vm] Covariance $\overline{v'm'}$ between $v$ wind component and molar concentration of methane in $\mbox{mmol}/\mbox{m}^{3}$ (present only if methane concentration is in corresponding raw data file).
 \item[wm] Covariance $\overline{w'm'}$ between $w$ wind component and molar concentration of methane in $\mbox{mmol}/\mbox{m}^{3}$ (present only if methane concentration is in corresponding raw data file).
 \item[um.nst] Non-steadiness of covariance $\overline{u'm'}$ (present only if methane concentration is in corresponding raw data file).
 \item[vm.nst] Non-steadiness of covariance $\overline{v'm'}$ (present only if methane concentration is in corresponding raw data file).
 \item[wm.nst] Non-steadiness of covariance $\overline{w'm'}$ (present only if methane concentration is in corresponding raw data file).
 \item[temp.avg] Mean temperature in Celsius degrees (present only if thermometer data is present in corresponding raw data file).
 \item[hrel.avg] Mean relative humidity in \% (present only if hygrometer data is present in corresponding raw data file).
 \item[u.trend.slope] Slope of linear trend of wind component $u$ (present only if \verb|trend.removal| is ``linear'').
 \item[u.trend.constant] Intercept of linear trend of wind component $u$ (present only if \verb|trend.removal| is ``linear'').
 \item[v.trend.slope] Slope of linear trend of wind component $v$ (present only if \verb|trend.removal| is ``linear'').
 \item[v.trend.constant] Intercept of linear trend of wind component $v$ (present only if \verb|trend.removal| is ``linear'').
 \item[w.trend.slope] Slope of linear trend of wind component $w$ (present only if \verb|trend.removal| is ``linear'').
 \item[w.trend.constant] Intercept of linear trend of wind component $w$ (present only if \verb|trend.removal| is ``linear'').
 \item[t.trend.slope] Slope of linear trend of sonic temperature (present only if \verb|trend.removal| is ``linear'').
 \item[t.trend.constant] Slope of linear trend of sonic temperature (present only if \verb|trend.removal| is ``linear'').
 \item[q.trend.slope] Slope of linear trend of water vapor (present only if \verb|trend.removal| is ``linear'' and water is present).
 \item[q.trend.constant] Slope of linear trend of water vapor (present only if \verb|trend.removal| is ``linear'' and water is present).
 \item[c.trend.slope] Slope of linear trend of carbon dioxide (present only if \verb|trend.removal| is ``linear'' and carbon dioxide is present).
 \item[c.trend.constant] Slope of linear trend of carbon dioxide (present only if \verb|trend.removal| is ``linear'' and carbon dioxide is present).
 \item[a.trend.slope] Slope of linear trend of ammonia (present only if \verb|trend.removal| is ``linear'' and ammonia is present).
 \item[a.trend.constant] Slope of linear trend of ammonia (present only if \verb|trend.removal| is ``linear'' and ammonia is present).
 \item[m.trend.slope] Slope of linear trend of methane (present only if \verb|trend.removal| is ``linear'' and methane is present).
 \item[m.trend.constant] Slope of linear trend of methane (present only if \verb|trend.removal| is ``linear'' and methane is present).
 \item[temp.trend.slope] Slope of linear trend of temperature (present only if \verb|trend.removal| is ``linear'' and temperature is present).
 \item[temp.trend.constant] Slope of linear trend of temperature (present only if \verb|trend.removal| is ``linear'' and temperature is present).
 \item[hrel.trend.slope] Slope of linear trend of relative humidity (present only if \verb|trend.removal| is ``linear'' and relative humidity is present).
 \item[hrel.trend.constant] Slope of linear trend of relative humidity (present only if \verb|trend.removal| is ``linear'' and relative humidity is present).
 \item[theta] First rotation angle (designated as $\alpha$ in description at \ref{sec:Eddy Covariance}), in degrees (this parameter is present only if mode == ``eddy.covariance'' or ``eddy.covariance.3'').
 \item[phi] Second rotation angle (designated as $\beta$ in description at \ref{sec:Eddy Covariance}), in degrees (this parameter is present only if mode == ``eddy.covariance'' or ``eddy.covariance.3'').
 \item[psi] Third rotation angle (designated as $\delta$ in description at \ref{sec:Eddy Covariance}), in degrees (this parameter is present only if mode == ``eddy.covariance'' or ``eddy.covariance.3''; also notice its value will be 0 if mode == ``eddy.covariance'').
 \item[alpha] Individual averaging block rotation angle used in planar fit (designated as $\alpha$ in description at \ref{sec:Eddy Covariance}), in degrees (this parameter is present only if mode == ``planar.fit'').
 \item[beta] First fitting plane rotation angle (designated as $\beta$ in description at \ref{sec:Eddy Covariance}), in degrees (this parameter is present only if mode == ``planar.fit'').
 \item[gamma] Second fitting plane rotation angle (designated as $\delta$ in description at \ref{sec:Eddy Covariance}), in degrees (this parameter is present only if mode == ``planar.fit'').
 \item[u.avg.rot] Component $u$ of mean wind in rotated frame (m/s).
 \item[v.avg.rot] Component $v$ of mean wind in rotated frame (m/s).
 \item[w.avg.rot] Component $w$ of mean wind in rotated frame (m/s).
 \item[Dir] Wind direction ( from North, povenance convention).
 \item[uu.rot] Covariance between $u$ and $u$ wind components in rotated frame ($\mbox{m}^2/\mbox{s}^2$).
 \item[uv.rot] Covariance between $u$ and $v$ wind components in rotated frame ($\mbox{m}^2/\mbox{s}^2$).
 \item[uw.rot] Covariance between $u$ and $w$ wind components in rotated frame ($\mbox{m}^2/\mbox{s}^2$).
 \item[vv.rot] Covariance between $v$ and $v$ wind components in rotated frame ($\mbox{m}^2/\mbox{s}^2$).
 \item[vw.rot] Covariance between $v$ and $w$ wind components in rotated frame ($\mbox{m}^2/\mbox{s}^2$).
 \item[ww.rot] Covariance between $w$ and $w$ wind components in rotated frame ($\mbox{m}^2/\mbox{s}^2$).
 \item[ut.rot] Covariance between $u$  wind components in rotated frame and temperature $t$ ($\mbox{m}/\mbox{s} K$).
 \item[vt.rot] Covariance between $v$  wind components in rotated frame and temperature $t$ ($\mbox{m}/\mbox{s} K$).
 \item[wt.rot] Covariance between $w$  wind components in rotated frame and temperature $t$ ($\mbox{m}/\mbox{s} K$).
 \item[u.star] Friction velocity ($\mbox{m}/\mbox{s}$).
 \item[H0.v] Buoyancy vertical flux ($\mbox{W}/\mbox{m}^2$; used as an approximation of the sensible heat flux).
 \item[L] Obukhov length ($\mbox{m}$).
 \item[z.over.L] Stability parameter (adimensional, with reference height assumed to be equal to the anemometer's).
 \item[T.star] Scale temperature (Celsius degrees).
 \item[Rho.air] Air density ($\mbox{g}/\mbox{m}^3$).
 \item[Rho.Cp] Value of $\rho \cdot C_{p}$ ($\mbox{J}/\mbox{m}^{3} \cdot \mbox{K}$).
 \item[Pa] Estimate of local atmospheric pressure, given altitude above mean sea level and temperature ($\mbox{hPa}$).
 \item[wq.rot] Covariance between wind component $w$ in rotated frame and molar concentration of water ($\mbox{m}\cdot\mbox{mmol}/\mbox{s}\cdot\mbox{m}^3$, that is $\mbox{mmol}/\mbox{s}\cdot\mbox{m}^2$; this field is present only if a water measurement is taken).
 \item[c.d] Molar concentration of air ($\mbox{mmol}/\mbox{m}^3$; this field is present only if a water measurement is taken).
 \item[q.d] Ratio of water molar concentration to air molar concentration (adimensional; this field is present only if a water measurement is taken).
 \item[c.dv] Molar concentration of moist air ($\mbox{mmol}/\mbox{m}^3$; this field is present only if a water measurement is taken).
 \item[t.c] Thermal term in WPL correction ($\mbox{mmol}/\mbox{m}^2 \cdot \mbox{s}$; this field is present only if a water measurement is taken).
 \item[Fq.molar] Molar flux of water along the vertical direction ($\mbox{mmol}/\mbox{m}^2 \cdot \mbox{s}$; this field is present only if a water measurement is taken).
 \item[Fq.mass] Mass flux of water along the vertical direction ($\mbox{mmol}/\mbox{m}^2 \cdot \mbox{s}$; this field is present only if a water measurement is taken).
 \item[mix.f] Conversion factor from  ($\mbox{mmol}/\mbox{m}^3$ to $\mbox{g}/\mbox{g}$; this field is present only if a water measurement is taken).
 \item[wt.cor] Value of $\overline{w'T'}$ where $T$ represents the real, not sonic temperature ($\mbox{m} \cdot \mbox{K}/\mbox{s}$; this field is present only if a water measurement is taken).
 \item[H0] Sensible heat flux ($\mbox{W} / \mbox{m}^2$; this field is present only if a water measurement is taken).
 \item[Lambda] Latent condensation heat for water ($\mbox{J}/\mbox{g}$; this field is present only if a water measurement is taken).
 \item[He] Latent heat flux ($\mbox{W} / \mbox{m}^2$; this field is present only if a water measurement is taken).
 \item[wc.rot] Covariance between wind component $w$ in rotated frame and molar concentration of carbon dioxide ($\mbox{m}\cdot\mbox{mmol}/\mbox{s}\cdot\mbox{m}^3$, that is $\mbox{mmol}/\mbox{s}\cdot\mbox{m}^2$; this field is present only if a carbon dioxide measurement is taken).
 \item[Fc.molar] Molar flux of carbon dioxide along the vertical direction ($\mbox{mmol}/\mbox{m}^2 \cdot \mbox{s}$; this field is present only if a carbon dioxide measurement is taken).
 \item[Fc.mass] Mass flux of carbon dioxide along the vertical direction ($\mbox{mmol}/\mbox{m}^2 \cdot \mbox{s}$; this field is present only if a carbon dioxide measurement is taken).
 \item[wa.rot] Covariance between wind component $w$ in rotated frame and molar concentration of ammonia ($\mbox{m}\cdot\mbox{mmol}/\mbox{s}\cdot\mbox{m}^3$, that is $\mbox{mmol}/\mbox{s}\cdot\mbox{m}^2$; this field is present only if an ammonia measurement is taken).
 \item[wm.rot] Covariance between wind component $w$ in rotated frame and molar concentration of methane ($\mbox{m}\cdot\mbox{mmol}/\mbox{s}\cdot\mbox{m}^3$, that is $\mbox{mmol}/\mbox{s}\cdot\mbox{m}^2$; this field is present only if a methane measurement is taken).
\end{description}

%*************************************************************************
\section{The object type ``dir.stat''}
\label{sec:dir.stat}

Objects of type \verb|dir.stat| are returned as result of the omonymous function \verb|dir.stat| upon successful completion, and are implemented as lists, with the following components:

\begin{description}
 \item[dir] Vector of original data directions (degrees).
 \item[data] Vector of original data values (unit depends on which value has been selected).
 \item[n.class] Number of direction classes (no unit).
 \item[c.dir] Vector of center directions corresponding to the direction classes (degrees).
 \item[conf] String, containing the name of the algorithm used to compute confidence limits (to date ``boot'' is the only supported option).
 \item[mean] Vector containing the mean values within each sector (unit depends on which value has been selected).
 \item[std.dev] Vector containing the standard deviation of values within each class (unit depends on which value has been selected).
 \item[min] Vector containing sector minima (unit depends on which value has been selected).
 \item[p.25] Vector containing sector 25-th percentiles (unit depends on which value has been selected).
 \item[median] Vector containing sector median (unit depends on which value has been selected).
 \item[p.75] Vector containing sector 75-th percentiles (unit depends on which value has been selected).
 \item[max] Vector containing sector maxima (unit depends on which value has been selected).
 \item[conf.min] Vector containing confidence interval minima for each sector (unit depends on which value has been selected).
 \item[conf.max] Vector Containing confidence interval maxima for each sector (unit depends on which value has been selected).
\end{description}


%*************************************************************************
\chapter{Data Sets}
\label{chap:Data Sets}

\section{CRA.01}
\label{sec:CRA.01}

\subsection{Data collection site}

\subsection{Meteorological context of data set period}

\subsection{Instrument configuration}
\label{sec:CRA.01 instrument configuration}

Data in CRA.01 set have been collected using a Metek USA-1 ultrasonic anemometer with analog input option and a LiCor LI-7500 open-path water and carbon dioxide sensor.

The two output channels of the LI-7500 have been so configured:

\begin{description}
 \item[Channel 1] Water. Low voltage (0V) corresponding to 0 $\mbox{mmol}/\mbox{m}^3$, and high (5V) to 1000 $\mbox{mmol}/\mbox{m}^3$.
 \item[Channel 2] Carbon dioxide. Low voltage (0V) corresponding to 10 $\mbox{mmol}/\mbox{m}^3$, and high (5V) to 20 $\mbox{mmol}/\mbox{m}^3$.
\end{description}

Channels 1 and 2 of the LI-7500 have then been connected to the USA-1 input channels 7 and 8 respectively.

The specific USA-1 used in data collection is equipped with a 16 bit analog-to-digital converter. Input voltages, in range -10 to +10V, are encoded to count values -32768 and 32767, reported as integer values. Range 0 to 5V encodes then to range 0 to 16384 counts.

The correspondence between count values and gas concentrations is expressed through a linear relation taking the form

\begin{equation}\label{eq:CRA.01 conversion}
 g = m \cdot C + q
\end{equation} 

\noindent where $m$ designates the multiplier, $q$ the offset, $C$ count readout from USA-1, and $g$ gas molar concentration.

Computing the values of $m$ and $q$ for water and carbon dioxide can be made using known information and assuming linear conversion as from (\ref{eq:CRA.01 conversion}). In particular:

\begin{description}
 \item[Water] $q$ [$\mbox{mmol}/\mbox{m}^3$] = $C \cdot (32767/10) \cdot (1000/5) = 0.0610370 C$. That is, $m=0.0610370$ and $q=0$,
 \item[Carbon dioxide] $c$ [$\mbox{mmol}/\mbox{m}^3$] = $C \cdot (32767/10) \cdot (20-10)/5 + 10 = 0.000610370 C + 10$. That is, $m=0.000610370$ and $q=10$,
\end{description}

LI-7500 analog output delay can be programmed, and users did select in this case the value 0.309s for each channel. The USA-1 has been operated at 20Hz, with automatic averaging every two samples, with an output dara rate hence equal to 10Hz. The delay selected then rounds effectively to 0.3s, that is, a three data packets.

(We notice incidentally that programming the LI-7500 analog outputs to tighten output band is common practice, since adopting blindly the instrument's factory defaults would resule in excessively large conversion interval and, as a consequence, in massive information loss. At many sites this loss is sufficient to devoid eddy covariance calculations of water and carbon dioxide fluxes of any physical meanings.)

\subsection{Data preparation}

Original data have been converted to SonicLib standard format in a two-stage process, 

\subsection{Where to find prepared data}


\section{UNIMI.Short.Mast and UNIMI.Long.Mast}

\subsection{Data collection site}

\subsection{Meteorological context}

\subsection{Instrument configuration}

\subsection{Data preparation}

\subsection{Where to find prepared data}

%*************************************************************************
\chapter{Converters}
\label{chap:Converters}

\section{Converting from SLT form}
\label{sec:Convert SLT}

SLT is a commonly used format used in \verb|EddySoft| program and its \verb|eddymeas| data acquisition system. Actually two variants of SLT exist, depending on whether data have been gathered by direct acquisition (\verb|eddymeas|) or by conversion from other formats.

SLT is a \emph{binary} format, with data encoded as integer numbers. The actual details of encoding is slightly different in the two variants, as shown by prof. W. Eugster in his \verb|readslt.R| program.

The SLT to SonicLib converter follows very closely prof. Eugster's implementation, with output file production instead of generation of an R data frame. The decision to use Fortran instead of R is mainly due to efficiency considerations, in the prospective of converting large bulks of existing data.

The converter is named \verb|readslt| in UNIX and Linux (or \verb|readslt.exe| under Windows). To use it you should open a terminal session under your operating system. Under UNIX and Linux the command line is

\begin{verbatim}
./readslt <InputPrefix> <DateTime> <Mode> <Convert> <OutputPath>
\end{verbatim}

\noindent where \verb|<InputPrefix>| is the name of the path containing data, followed by the first letters (the alphabetic, not numerical, ones) in data files you're interested in (this is necessary, as the first letters in file name are to some extent freely assigned).

\verb|<DateTime>|, as its name suggests, designates the date and time of the data you like to convert, in ISO form (``\verb|YYYY-MM-DD hh:00:00|''), with minute and seconds forced to zero. This, because we're interested in a specific \emph{hour} in SonicLib, while SLT files are often splitted in fractions of a hour. This argument must be enclused in double quotes, as it contains a space: without quotes, the command line interpreter would split this single argument in two.

\verb|<Mode>| is an alphanumeric string, whose values can only be \verb|eddysoft| and \verb|eddymeas|, specifying any of the two available forms.

Parameter \verb|<Convert>| is the name of a file containing conversion specifications for all data in columns 5 to last, containing analog values. Something more further on.

The last parameter, \verb|<OutputPath>|, contains the name of the directory where the SonicLib file resulting from conversion will be written to.

\subsection{The SLT conversion specification file}

One hard problem of SLT files is they encode data in integer form, and this includes analog data. SonicLib files, on the contrary, contain floating point data.

\section{Converting from Meteoflux Core V1.0}
\label{sec:Convert MFC1}

\section{Converting from Meteoflux Core V2.0}
\label{sec:Convert MFC2}

\section{Converting from old Meteoflux V3.x}
\label{sec:Convert MFX3}

\section{Converting from LabVIEW-made data acquisition systems}
\label{sec:Convert LV}

\subsection{Little- and high-endianness}

\subsection{Endianness in LabVIEW is fixed}

\subsection{Long (32 bit) and short (16 bit) data forms from Servizi Territorio}


\section{Converting from GRIFO form}
\label{sec:Convert GRIFO}


%*************************************************************************
\chapter{Data Gathering and Preparation}
\label{chap:Data Gathering}

This chapter presents some functions whose use allows accessing data in standard format, and their preparation for further processing.

Individual functions are described in sections of this chapter. Each function-specific description follows the same form, 

\section{Read Single Standard Ultrasonic Anemometer Raw Data File}
\label{sec:Get Raw Data File}

\subsection{Function Use}

To read a single standard ultrasonic anemometer data file use function

\begin{verbatim}
get.raw.data(file.name, sampling.rate=10,
     threshold=0.005, verbose=FALSE)
\end{verbatim}

\noindent where

\begin{description}
 \item[file.name] is the name of file to read, according to the specification given in section \ref{sec:FileFormat}.
 \item[sampling.rate] is an integer number indicating the sampling rate $f_{s}$ in samples per second adopted when collecting all data in file, according to user knowledge. This value is used to compute the expected number of data in file as $n_{s} = 3600 \cdot f_{s}$; the expected number of data is compared with the number of data actually present in file, $n$ and if the relative difference $(n-n_{s})/n_{s}$ is smaller than a threshold value (see next parameter) the data set is refused. Default = 10.
 \item[threshold] Value of the relative difference $(n-n_{s})/n_{s}$ between actual and expected number of data below which data set is refused. Default = 0.005.
\item[verbose] Boolean flag. If TRUE, error and progress message are printed to screen (most useful in interactive use). If FALSE, no message is printed and the success can be assessed only by inspecting output value.
\end{description}

The routine yields a result which on error is NULL (an error message is printed in case only if ``verbose'' parameter is TRUE). If data read has been completed successfully result is an object of type ``sonic.raw.data'', described in \ref{sec:FileFormat}.

\subsection{Examples}

In our first example a data file is read and its $w$ component estimated density plotted to graphic window. A normal density of equal parameters is also plotted as a dashed line, and the result presented in figure \ref{fig:get.raw.data.1}.

\begin{verbatim}
> d <- get.raw.data("../TestData/20100516.10.csv", verbose=TRUE)
>
> plot(density(d$data$w, na.rm=TRUE), 
       xlab="w (m/s)", ylab="Density", main="")
>
> # Build a normal density with same mean
> # and standard deviation and add to plot
> mu <- mean(d$data$w, na.rm=TRUE)
> sigma <- sqrt(var(d$data$w, na.rm=TRUE))
> w.min <- min(d$data$w, na.rm=TRUE)
> w.max <- max(d$data$w, na.rm=TRUE)
> x <- seq(from=w.min, to=w.max, length.out=128)
> lines(x,dnorm(x,mu,sigma),lty="dashed")
\end{verbatim} 

\begin{figure}[htp]
 \centering
 \begin{center}
 \includegraphics[scale=1.1,keepaspectratio=true]{./diagrams/get_raw_data_1.pdf}
 % get.raw.data.1.pdf: 288x180 pixel, 72dpi, 10.16x6.35 cm, bb=0 0 288 180
 \end{center}
 \caption{Result from first example; normal density (dashed line) shown for reference}
 \label{fig:get.raw.data.1}
\end{figure}

Quantifying data availability in a single file is an often-performed task, to which no SonicLib function has been devoted. The reason is, deciding whether a data item is valid (and then contributing to availability) or not includes a bit of subjectivity. And more important, testing availability within a hourly file is usually very simple, and may be accomplished by directly using R capabilities.

In the next example availability is checked by counting how many data have been labeled ``valid'' by the ultrasonic anemometer, and comparing the result with the total number of (valid and invalid) data lines. The result is expressed in percent form.

\begin{verbatim}
> d <- get.raw.data("../TestData/20100516.10.csv", verbose=TRUE)
> 100 * sum(!is.na(d$data$w)) / length(d$data$w)
  [1] 100
\end{verbatim} 

100\% of data availability within a single hour is quite common with ultrasonic anemometers, thanks to their immunity to  wind calms.

\subsection{Notes}

\begin{itemize}
 \item If optional (e.g. water or carbon dioxide) columns are present they are read with no applied time shift, so that it may be estimated a posteriori if necessary.
 \item All data read are not scaled, all changes necessary to transform from original instrument files and sonic standard raw files having been made previously by adapter procedures, fed with appropriate conversion parameters. 
\end{itemize}


\section{Read Multiple Standard Ultrasonic Anemometer Raw Data Files}
\label{sec:Get Multiple Raw Data Files}

\subsection{Function Use}

To read a single standard ultrasonic anemometer data file use function

\begin{verbatim}
get.multi.raw.data(file.name, n.hours=1, sampling.rate=10,
     threshold=0.005, average.by="none", verbose=FALSE)
\end{verbatim}

\noindent where

\begin{description}
 \item[file.name] is the name of first file to read, according to the specification given in section \ref{sec:FileFormat}.
 \item[n.hours] is the number of hours to stich together (default: 1).
 \item[sampling.rate] is an integer number indicating the sampling rate $f_{s}$ in samples per second adopted when collecting all data in file, according to user knowledge. This value is used to compute the expected number of data in file as $n_{s} = 3600 \cdot f_{s}$; the expected number of data is compared with the number of data actually present in file, $n$ and if the relative difference $(n-n_{s})/n_{s}$ is smaller than a threshold value (see next parameter) the data set is refused. Default = 10.
 \item[threshold] Value of the relative difference $(n-n_{s})/n_{s}$ between actual and expected number of data below which data set is refused. Default = 0.005.
 \item[average.by] String, whose allowed values are ``none'' (default), ``seconds'' and ``minutes''; if ``none'', data are gathered as they are, without any averaging; if ``seconds'', data are averaged on one second basis; if ``minutes'', data are averaged on 1 minute basis.
 \item[verbose] Boolean flag. If TRUE, error and progress message are printed to screen (most useful in interactive use). If FALSE, no message is printed and the success can be assessed only by inspecting output value.
\end{description}

The routine yields a result which on error is NULL (an error message is printed in case only if ``verbose'' parameter is TRUE). If data read has been completed successfully result is an object of type ``sonic.raw.data'', described in \ref{sec:FileFormat}.

\subsection{Examples}

Function \verb|get.multi.raw.data| is typically used in preparation of spectral analysis, when multi-hour files are to be packed together in order to gain band on the left side of log-log diagrams customarily used in micro-meteorology.

The following example shows an example of such a preparation (without the actual spectral part).

\begin{verbatim}
> d <- get.multi.raw.data("../TestData/20100516.00.csv", n.hours=4, verbose=TRUE)
\end{verbatim} 

Proceding this way, and accepting all default, we requested in fact to get four data files and stitch them one after the other, without any averaging (the default value of \verb|average.by| is \verb|"none"|). Operating at 10 Hz this will produce a data set whose length will be around $36000 \cdot 4 = 144000$, quite huge for even the fastest FFT (which if data size is not a power of 2 may degrade performance from order $n \cdot \log n$ to $n^{2}$. More important, the result will be very noisy as information from many frequency scales is all mixed together.

A sensible approach might be to average data every seconds, improving the overall appearance of the low frequency part of spectral diagram, but losing an order of magnitude on the Nyquist frequency (from 5 to 0.5 Hz). The call with per-second averaging is:

\begin{verbatim}
> d <- get.multi.raw.data("../TestData/20100516.00.csv", n.hours=4, average.by="seconds", verbose=TRUE)
\end{verbatim} 

\subsection{Notes}

Function \verb|get.multi.raw.data| is designed to stitch together a \emph{reasonable} number of data files, say from 1 to 8. Averaging is done after all raw data are read and stitched together, even if per-second or per-minute averaging is made. This will result in significant memory occupation if the number of hours to get is high.

For this function to complete successfully it is required that all individual data files exist, and contain at least $100 \cdot (1-\mbox{threshold})$ percent of data. This may seem restrictive (at least if the small default threshold value is retained), but it helps spectral calculations, for which the function has mostly have been written: allowing a high number of gaps would in case force to employ costly periodogram-based algorithms instead of lightweight FFTs. Of course, the authors opinion is just an opinion, and you, the user, are entirely free of choosing your sensible threshold value: you go, gal - at your own risk.


\section{Extract Subset from a Standard Ultrasonic Anemometer Data Set}
\label{sec:Extract Subset from Raw Data Set}


\section{Average Data from a Single Raw Data File}
\label{sec:Average Single Raw Data File}

\subsection{Function Use}

To average data from a single raw data file use this function:

\begin{verbatim}
average.sonic.data(
  d, 
  initial.stamp,
  averaging.time=30,
  delay=0.3,
  trend.removal="none",
  verbose=FALSE
)
\end{verbatim} 

\noindent where

\begin{description}
 \item[d] is an object of type \verb|sonic.raw.data| on which averages are taken.
 \item[initial.stamp] is a POSIXct date and time associated to the beginning instant the data in file have been gathered; this date and time is not arbitrary, and may be generated from file name \emph{in normal cases}. In some circumstances, however, it may be useful to shift time forward or backward by some fixed time, for example to adjust a systematic error of data acquisition system clock. By the way: the POSIXct date and time corresponding to file name may be obtained through function \verb|time.stamp.from.name|, whose use is described later on as an example.
 \item[averaging.time] is a positive integer $t_{a}$, whose admissible values are 10, 15, 20, 30, 60, indicating the averaging time to be used. Default: 30.
 \item[delay] A real number indicating a unique delay value to applied to any optional columns simultaneously, or an object of class ``delay.spec'' indicating the delay used for any possible optional column. Objects of class ``delay.spec'' are created using function \verb|set.delay|, whose use is described in the examples. Default: 0.3 s.
 \item[trend.removal] is a string specifying the trend removal algorithm to use. Possible values are ``none'' (no trend removal is performed), ``linear'' (trend is estimated by linear least squares regression and removed in mean-preserving fashion). The default is ``none''.
 \item[verbose] A boolean flag indicating whether messages are to be printed (TRUE) or not (FALSE).
\end{description}

Function \verb|average.sonic.data| returns on exit a NULL value in case of failure, or an object of class \verb|sonic.avg.data|, described in section \ref{sec:sonic.avg.data}.

\subsection{Time stamps: needed or not in data files?}

Section \ref{sec:sonic.avg.data} lists \verb|time.stamp| among the ``mandatory'' columns. But a word of caution is necessary here, as things are somewhat less dramatic.

In fact: section \ref{sec:sonic.avg.data} deals with the SonicLib raw data files \emph{as they look after they have been read into memory}. This is not exactly the same as how the very same data appear \emph{on disk}.

To date the only difference between SonicLib raw data files on disk and their memory representation is the time stamp, which may be missing from disk files.

In this case, the time stamp is inferred from data using the rule

\begin{equation}\label{eq:Inferred time stamp}
 t = 3600 \cdot \frac{i-1}{n}
\end{equation}

\noindent where $i$ is the data item index and $n$ the total number of data items in file. You may notice from \ref{eq:Inferred time stamp} that time stamp is stored as a number in interval $0 \le t < 3600$.


\subsection{Trend removal}

One common manifestation on non-stationarity in data is the occurrence of a slow variation in data, as if their mean value drifts following some smooth function of time.

This slow variation is visible within any averaging interval, and may be approximated by a linear function.

As we're using time averages and covariances within each averaging block, if this slow variation is not removed its value will leak into covariances, and appear as a fictive turbulent flux.

It is worth noticing the adverse effects of linear trend are a direct consequence of the way we compute the means and covariances as basically time averages. For them to make sense in the context of data processing of sonic anemometer measurements it is necessary that time averages and ensemble means coincide, which is possible only if we assume stationarity in the process underlying data, and in data themselves. The presence of a linear trend directly invalidates this assumption and, with it, any further serious consideration.

The problem then arises, on how to remove linear trends from any averaging block in sonic data series.

One solution assumes data are positioned in time with sufficient precision. Then, we may take all couples $(t_{i},s_{i})$, where $s$ indicates signal values and $t_{i}$ the corresponding time instant, and treat them as experimental points. Them given, we want to identify the corresponding linear model

\begin{equation}\label{eq:Trend Concept}
 s = a \cdot t + b + \varepsilon
\end{equation}

\noindent where $\varepsilon$ is a model error.

One popular way to identify model (\ref{eq:Trend Concept}) is by minimizing the squared difference between experimental points and model values at the corresponding instant. This linear least squares regression yields the equation of a line $s(t) = a \cdot t + b$ which is an estimate of our trend.

Now, if we limit to subtract the trend estimate from the original signal, we obtain

\begin{equation}\label{eq:Mean changing trend removal}
 S_{i} = s_{i} - a \cdot t_{i} - b
\end{equation}

noindent whose average is

\begin{equation}
 \overline{S} = \frac{1}{n} \sum_{i=1}^{n} S_{i} = \frac{1}{n} \sum_{i=1}^{n} s_{i} - a \cdot \frac{1}{n} \sum_{i=1}^{n} t_{i} - b = \overline{s} - a \cdot \overline{t} - b
\end{equation}

\noindent which is not the original $\overline{s}$, nor in general has some desirable property as being zero. So, within SonicLib liner trend removal is \emph{mean-preserving}: after the linear trend has been removed using (\ref{eq:Mean changing trend removal}) the new signal mean is computed and subtracted from the transformed signal. Then, the mean of original signal is added back. In other terms:

\begin{equation}\label{eq:Mean preserving trend removal}
 S_{i} = s_{i} - a \cdot t_{i} - b - \overline{S} + \overline{s}
\end{equation}

Changing the trend-freed signal so that its mean is preserved has no effect on covariances, meanwhile allowing to perform average-depending computations (for example finding rotation angles in eddy covariance).

For all this to be possible, however, we need each data item is accompanied by a non-ambiguous time stamp $t_{i}$ In section \ref{sec:SonicLib Raw File Time Stamps} we have seen this is not necessarily the case: all we may be reasonably sure is time stamps are ``regular''.

Time stamp regularity is just fine if all we need to do is partitioning a signal collected in an hourly file among various averaging blocks. But regrettably it is not sufficient to compose experimental points suitable to linear regression. As a workaround, and only for trend removal, an equally-spaced time stamp is synthetized from index by applying the following rule:

\begin{equation}
  t_{i} = \delta t \cdot (i-1)
\end{equation}

\noindent where $\delta t = 1 / f_{s}$.

It is worth mentioning this trend removal strategy is effective only when trend originates from a smooth change as temperature's normal daily variation. It is completely worthless in cases where the trend takes for example the form of a sudden change, like when a cold or warm front breaks in, or at beginning of Foehn episode.

Another more subtle point is that trend removal as done in SonicLib is entirely ``statistical'': it is always possible to perform it, but it has no ``real'' physical meanings. The trend a human eye and brain can detect, however, are of entirely physical origin. An ideal trend removal strategy should take this into proper account. In SonicLib, at least in the moment, we don't.

\subsection{Spike removal}



\subsection{Examples}
\subsubsection{Simple data averaging}

In many cases all what done is just averaging data the simple way, accepting all defaults. This is accomplished by issuing a sequence like this:

\begin{verbatim}
> file.name <- "../TestData/20100516.10.csv"
> d <- get.raw.data(file.name, verbose=TRUE)
> d.stat <- average.sonic.data(d, time.stamp.from.name(file.name))
\end{verbatim} 

Result, stored in variable \verb|d.stat|, contains one record per each of the two 30 minutes block within hour.

\subsubsection{Using different delays for different variables}

When more than one analog data set is in raw data file it may happen their data acquisition delays to differ. If this is the case, a unique real valued delay (like the default value) cannot be used, and function \verb|set.delay| is to be used.

The following example shows how different delays are used, assuming a LiCor 7500 water and carbon dioxide detector and a Vaisala HMP45 thermo-hygrometer are connected to ultrasonic anemometer's analog input channels (and then, available in files):

\begin{verbatim}
> dly <- set.delay(q=0.3, c=0.3, temp=0.0, hrel=0.0)
> file.name <- "../TestData/20120401.03.csv"
> d <- get.raw.data(file.name, verbose=TRUE)
> d.stat <- average.sonic.data(d, time.stamp.from.name(file.name),
                delay=dly)
\end{verbatim} 


\subsection{Notes}

Function \verb|average.sonic.data| is seldom used directly, as most data processing occurs on entire campaigns more than a unique file. It may however happen access to data in a single hour is needed (maybe for diagnostic purposes), and then it makes sense to use \verb|average.sonic.data| on its own.

Averages and covariances are computed according to the usual definitions, that is blockwise. No weighting nor pre-filtering strategy is employed during averaging (although signal preparation may have been made priori of this step).

The relative non-stationarity of wind component $x$ is defined as

\begin{equation}\label{eq:Relative Nonstationarity of X}
 RN_{x} \doteq \frac{\delta x}{\overline{x}}
\end{equation} 

\noindent as defined in \cite{Vickers1997}. Non-steadiness of covariance $\overline{x'y'}$ is defined according to \cite{Foken1996} as

\begin{equation}\label{eq:Non Steadiness of xy}
 NS_{\overline{x'y'}} \doteq \frac{\hat{x'y'} - \overline{x'y'}}{\overline{x'y'}}
\end{equation} 

\noindent where

\begin{equation}\label{eq:Partial Covariance}
 \hat{x'y'} \doteq \frac{1}{n} \sum_{i=1}^{n}\overline{x'y'}_i
\end{equation} 

The symbol $\overline{x'y'}_i$ designates the \emph{i}-th 5 minutes block within averaging time. $n$, the number of blocks, may be computed readily as $\frac{t_{a}}{5}$, where $t_{a}$ represents the averaging time in minutes. Note $n$ is an integer, because of the restrictions imposed on allowed values of $t_{a}$.


\section{Average Data from Multiple Raw Data Files}
\label{sec:Average Multiple Raw Data Files}

\subsection{Function Use}

Computing averages is most often done on the result of whole measurement campaigns. If these are stored in a single file, then averaging may be performed using function

\begin{verbatim}
average.sonic.file.set(
  dir.name=".",
  time.zone="UTC",
  shift=0,
  averaging.time=30,
  sampling.rate=10,
  threshold=0.1,
  delay=0.3,
  trend.removal="none",
  verbose=FALSE
)
\end{verbatim} 

\noindent with the following argument meaning:

\begin{description}
 \item[dir.name] Name of directry where data are looked for (default: current directory).
 \item[time.zone] String containing the standard name of the time zone to which all time stamps will be attributed (default: ``UTC''). 
 \item[shift] Number of seconds to be added to time stamps to obtain ``true'' time, used when mass-adjusting a data set (default: 0).
 \item[averaging.time] Value of averaging time to be used when processing data in set, in minutes (default: 30; allowed values: 10, 15, 20, 30, 60).
 \item[sampling.rate] Sampling rate assumed for data in set, in Hertz (default: 10).
 \item[threshold] Fraction of missing or invalid data above which an averages record is considered statistically invalid, and assigned the NA value (default: 0.1).
 \item[delay] A real number indicating a unique delay value to applied to any optional columns simultaneously, or an object of class ``delay.spec'' indicating the delay used for any possible optional column. Objects of class ``delay.spec'' are created using function \verb|set.delay|, whose use is described in an examples of section \ref{sec:Average Single Raw Data File}. Default: 0.3 s.
 \item[trend.removal] is a string specifying the trend removal algorithm to use. Possible values are ``none'' (no trend removal is performed), ``linear'' (trend is estimated by linear least squares regression and removed in mean-preserving fashion). The default is ``none''.
 \item[verbose] Boolean flag indicating whether output messages should be printed (TRUE) or not (FALSE).
\end{description}

In case of failure a NULL value is returned.

If averaging terminated correctly then an object of class \verb|sonic.avg.data| is returned (see section \ref{sec:sonic.avg.data}).

\subsection{Troubleshooting hints}

Function \verb|average.sonic.file.set| is most likely your first contact with SonicLib: most of processing aimed at ``distilling information from sonic data'' starts from averaging out a data set, and this in turn involves one or more calls to \verb|average.sonic.file.set| as the very first step.

Unfortunately, \verb|average.sonic.file.set| is also quite a delicate step. In itself is not so dramatically difficult from, say, a mathematical or computer science standpoint. But on the other side, it interfaces with ``standard SonicLib raw data files'', the product of a heterogeneous set of converters in quick evolution.

Chances are non-zero that the last minute version of that new shiny converter does not really produce so ``standard'' SonicLib raw data files. This is something the kernel SonicLib and its developers can not directly control, converters often coming from end users.

In cases like these \verb|average.sonic.file.set| stops on error. If parameter \verb|verbose| is set to \verb|TRUE| messages are printed in case of troubles: they may be illuminating but understanding them is much quicker with a little bit of a-priori information.

In the subsections following each of the important error messages is described in its meanings. Before to proceed, we note messages may come from \verb|averaging.sonic.file.set|, \verb|averaging.sonic.file| and \verb|get_raw_data|, that is the function described here and the functions it calls in turn: in the first case the error is likely to reside in something pertaining the file set as a whole, while in the others some specific error on a single data file has occurred (in fact, all messages but one come from the single file command \verb|averaging.sonic.file|).

\subsubsection{Message average.sonic.file.set:: Error: No CSV files in directory}

SonicLib raw data files are CSV file, and their extension must be ``.csv''. If the data directory contains no file with ``.csv'' extension then there is no point for \verb|averaging.sonic.file.set| to do anything.

The most likely cause of this message is you have specified a wrong directory name. This may be because the directory does not exist, or it exists but really contains no CSV file.

Notice that if at least a CSV file exists as far as \verb|averaging.sonic.file.set| is concerned it's all right. The message is not emitted and, as soon as \verb|averaging.sonic.file.set| will try reading the wrong CSV file some other error will occur.


\subsubsection{Message average.sonic.data:: error: Argument 'delay' is not a member of an instance of class ``delay.spec``}

This message means you have specified a delay of the ''wrong type``. Delays, used to cope with the small retard analog data sampled from sensors connected to ultrasonic anemometers often exhibit, can be only numbers (in which case the same delay value is applied uniformly to all channels), or lists of class ''delay.spec``).

Anything else would make \verb|averaging.sonic.file.set| quite hysterical; to avoid this possibility a type check is made and in case of failure the message is printed.

If you see this message just check parameter \verb|delay|.

\subsubsection{Messages average.sonic.data:: error: Averaging time is not a multiple of 5 minutes, average.sonic.data:: error: Averaging time is not a divisor of 60 minutes, Message average.sonic.data:: error: Averaging time is not between 10 and 60 minutes}

All these messages are self-explaining: averaging time must be a multiple of 5 minutes dividing exactly one hour and not smaller than 10 minutes.

In practice, the values allowed for the averaging time are 10, 15, 20, 30 and 60 minutes.

If the value of the averaging time is set to a number not satisfying any of these conditions one of the messages is emitted.

\subsubsection{Message get.raw.data:: Error: No columns in input data}

This message basically tells SonicLib did not understand what the content of a file is.

The most common reason is the file is not a SonicLib raw data file, although its type is CSV. From a purely technical standpoint this may mean the file has been prepared using a spreadsheet and exporting to CSV, but the original file did not contain tabular data.

In practice, the meanings is the directory assumed to contain only SonicLib raw data also contains something else, and of a very strange nature.

An important thing to notice is that despite of the ''error`` nature of this message the function \verb|averaging.sonic.file.set| does not stop upon encountering it: the offending file is just skipped and nothing bad happens (apart from the message).

Anyway, if this error annoys you the best you can do is moving the wrong file somewhere else. Locating it is quite simple: before printing the error message SonicLib dumps the name of the current file to the R concole.

It is not unworthy to stress that SonicLib expects the directory containing raw data files is ''clean``, that is, contains nothing different than SonicLib raw data files.

\subsubsection{Message get.raw.data:: Error: Number of data inconsistent with the expected number}

This message in practice tells SonicLib was able to find something tabular in a data file, but the number of rows does not match with the expected number.

This latter is assumed equal to $n_{s} \doteq f_{s} \cdot 3600$, where $f_{s}$ is the sonic's data acquisition frequency employed when collecting current data file. Given $n$, the number of data records actually present in file, the following indicator is formed:

\begin{equation}\label{eq:F indicator}
  F = \frac{\left| n - n_{s} \right|}{n_{s}} - 1
\end{equation}

If $F$ is smaller than the threshold, the corresponding data file is rejected. $F$ represents the deviation of total data in file from the number of expected data, expressed in fraction form. Because of the absolute value in \ref{eq:F indicator} the check operates both when the number of actual data is smaller than expected, and when it's \emph{larger}. This choice allows to trigger attention on data sets whose acquisition parameters differ from the ones assumed, which in many cases are just left to their default values: if this message occurs on any file in the data set, then you may be sure the assumed sampling frequency is wrong.

Assigning a sensible value to the threshold may involve contextual data. As a general rule, however, a tighter threshold might make sense when averaging and trend removal are made, or when an averaging period of less than an hour is adopted (regardless on trend removal state). The default in \verb|averaging.sonic.file.set|, 0.05, reflects this.

\subsubsection{Message get.raw.data:: Error: No 'u', 'v', 'w' or 't' data in file}

This message tells that at least one of the mandatory columns expected in a SonicLib file is not present, and as a consequence the file is rejected.

A likely cause of this error is the supposed SonicLib raw data file is not really a SonicLib file. Or, the column names within of it are wrong (this \emph{may} happen: SonicLib raw data file columns are spelled in lowercase, and maybe the converter wrote them in uppercase).

It's interesting to note the time stamp is not mentioned in this message. The reason is, time stamp is generated automatically if missing from data files. But: for this approach to make sense the file must be reasonably free from gaps, that is, $F$ must be smaller than the (tight) threshold.

\subsection{Examples}
\subsubsection{Averaging data ``acritically''}

Function \verb|average.sonic.file.set| may be used to just load data and average them on the fly, maybe within an automatic offline data processing script like in this example:

\begin{verbatim}
d <- average.sonic.file.set(
  "../TestData",
  delay=0.3, verbose=FALSE
)
\end{verbatim} 

\subsubsection{Preliminary assessment of data quality}

The result of preceding call may then be used directly to feed data processing functions like \verb|eddy.covariance|, as will commonly be made by many users (and automatic procedures).

But if we desire, we can do some more: use averaging and its associated calculation of relative nonstationarity and non-steadiness indices as a direct exploratory tool to preliminarily assess data quality.

In the next example we use averaging to check which hourly files are affected by significant non-stationarity on wind component $w$ and sonic temperature $theta$.

This makes sense if we desire to compute turbulent heat flux from sonic anemometer data reliably, and we can afford the risk of rejecting low-quality data sets.

Of course things are not everytimes this lucky way. For example, if our objective is to feed a lagrangian dispersion model in real time then it could make better sense to be less finicky with data, and accept hours characterized by a sub-standard quality: in cases like this it may be better to use available measurements, accepting the risk some of them are not ``scientific paper strength'', instead of resorting to data imputation strategies (that is, invention of fictive but ``plausible'' data to fill gaps). As almost always happens with data processing, the ``best'' way depends very much on the context.

So, let's assume we want to collect the subset of data on which e.g. eddy covariance will likely get best results, maybe in view of a scientific publication. Because of the assumptions underlying eddy covariance, results tend to be better when data are ``stationary''.

In a real data set, however, stationarity will never be really fulfilled. The requirement of stationarity is simply too restrictive, and no natural data set will lack its part of trends, changes (often abrupt) of variance or covariance, and infinite more.

So pragmatically we define a hourly file to be ``stationary enough'' if relative non-stationarity of $w$ is, say, less than 0.5 and the non-steadiness of $\overline{w'\theta'}$ less than 0.3.

Our first step is averaging data. But as we're interested in assessing quality of \emph{hourly} files we avoid using the default of \verb|averaging.time|, 30 minutes, and specify 60 minutes instead:

\begin{verbatim}
d <- average.sonic.file.set(
  "../TestData", averaging.time=60,
  delay=0.3, verbose=FALSE
)
\end{verbatim} 

\noindent Actual plotting may then be made with standard R commands, like this:

\begin{verbatim}
plot(
  d$data$t.stamp,
  d$data$w.avg,
  xlab="",
  ylab="Non-rotated W (m/s)",
  main="",
  type="l"
)
\end{verbatim} 

Here incidentally we see how convenient it is having adopted R for developing SonicLib: in addition to SonicLib specific function (in fact, a tiny set), we gain access to the immense graphic and data analysis facilities already built into R.

Taking this for granted, we may then plot the time evolution of relative non-stationarity and non-steadiness we're interested in, along with the corresponding limiting values we've selected so far. The result is shown in figures \ref{fig:average.sonic.file.set.1} and \ref{fig:average.sonic.file.set.2}.

But before of this we may do a simpler check, directly on available data. In our case we may learn quite a bit just plotting the hourly averages of vertical wind velocity. The importance of this and other similar ``simple'' checks can not be overlooked: in our specific case, finding a significantly non-zero vertical mean before applying axis rotation may indicate some positioning problem, and more so if all values have the same sign, thus indicating some systematic trouble.

In our data set, ``CRA.01'', this is precisely what happens, as we may see in figure \ref{fig:average.sonic.file.set.0} where we see all non-rotated hourly means of vertical speed to be negative, and indeed quite large. Why does this happen? Is this related to some specific provenance direction, or happens in all cases? Answering these and other related questions is fundamental when documenting micro-meteorological campaigns, and often involves both describing site conditions and performing exploratory data processing before embarking in the huge processing work.

\begin{figure}[htp]
 \centering
 \begin{center}
 \includegraphics[scale=1.1,keepaspectratio=true]{./diagrams/average_sonic_file_set_0.pdf}
 % average.sonic.file.set.1.pdf: 288x180 pixel, 72dpi, 10.16x6.35 cm, bb=0 0 288 180
 \end{center}
 \caption{Time evolution of hourly-averaged wind component $w$ in data set ``CRA.01''}
 \label{fig:average.sonic.file.set.0}
\end{figure}

\begin{figure}[htp]
 \centering
 \begin{center}
 \includegraphics[scale=1.1,keepaspectratio=true]{./diagrams/average_sonic_file_set_1.pdf}
 % average.sonic.file.set.1.pdf: 288x180 pixel, 72dpi, 10.16x6.35 cm, bb=0 0 288 180
 \end{center}
 \caption{Time evolution of relative non-stationarity of wind component $w$ in data set ``CRA.01''; note the threshold value is negative, as all averages of $w$ are lower than 0 in the test data set considered}
 \label{fig:average.sonic.file.set.1}
\end{figure}

\begin{figure}[htp]
 \centering
 \begin{center}
 \includegraphics[scale=1.1,keepaspectratio=true]{./diagrams/average_sonic_file_set_2.pdf}
 % average.sonic.file.set.2.pdf: 288x180 pixel, 72dpi, 10.16x6.35 cm, bb=0 0 288 180
 \end{center}
 \caption{Time evolution of non steadiness of covariance $\overline{w'\theta'}$ in data set ``CRA.01''}
 \label{fig:average.sonic.file.set.2}
\end{figure}

As we may see from figures \ref{fig:average.sonic.file.set.1} and \ref{fig:average.sonic.file.set.2}, most hours are ``good'' (in the strict sense that both relative non-stationarity of $w$ and non-steadiness of $\overline{w'\theta'}$ are below their respective thresholds. But some cases exist when one or both of these diagnostic values exceed the threshold.

What to do in these cases?

As mentioned before, no best a priori method can be devised, and a sensible answer must take the overall context into proper account. If, as we imagined, the purpose of the measurement campaign is to write a scientific paper stating some new truth, then it may make sense to \emph{exclude} from any further use the hours whose non stationarity or non steadiness exceed their thrsholds.

Space restrictions do not allow us to proceed further, but an analysis might be worth on rejected data to see whether they really \emph{are} non-stationary. And if so, \emph{why}. Exceptions in data are very often revealing of something deep and unexpected, and it would be a little pity having to just not use them. These detailed analyses, in any case, are highly exploratory and visual, and here R really shines as a life-saving helper.


\subsection{Notes}

Function \verb|average.sonic.file.set| assumes argument \verb|dir.name| to specify a directory in which data files in standard SonicLib format are found, but this does not mean they are required to be unique, nor that some of them are completely missing.

To avoid confusion however it is highly recommended to avoid placing in the data directory anything different from data, and to separate data from different campaigns in different directories. Function \verb|average.sonic.file.set| continues to operate normally in these cases, but results are homogeneous and without strange surprises - at least, strangeness stems from data themselves.

%*************************************************************************
\chapter{Giving Data a Meaning}
\label{chap:Data Interpretation}

\section{Eddy Covariance}
\label{sec:Eddy Covariance}

An important part of ultrasonic anemometer data processing has to do with computing fluxes of matter and energy, in contexts as diverse as air quality monitoring and ecosystems ecology. As different as they may be, all these application have one thing in common: they rely on some form of eddy covariance.

The SonicLib function implementing eddy covariance is

\begin{verbatim}
eddy.covariance(
  d,
  station.altitude,
  anemometer.height,
  mode,
  third.rotation.angular.threshold,
  verbose
)
\end{verbatim} 

\noindent where

\begin{description}
 \item[d] is an object of type \verb|sonic.avg.data| on which eddy covariance calculations are to be made.
 \item[station.altitude] is the station height above mean sea level, in meters (no default).
 \item[anemometer.height] is the anemometer height above ground level, in meters (default=10).
 \item[mode] is a string specifying the axis rotation method to use; possible values are ``eddy.covariance'' (classical eddy covariance with 2 axis rotations, also the default), ``eddy.covariance.3'' (eddy covariance with three axis rotations) and ``planar.fit''.
 \item[third.rotation.angular.threshold] Positive real number which is compared to third rotation angle. If the absolute value of third rotation angle is found larger than threshold value the third rotation is not performed. Default: 10. Expressed in degrees.
 \item[verbose] is a boolean flag indicating whether output messages should be printed (TRUE) or not (FALSE).
\end{description}

In case of error a result value of NULL is returned. In case of successful return, an object of type \verb|sonic.eddy.cov.data| is returned.

\subsection{Preliminary estimates}

An important part of ``eddy covariance'' consists in applying similarity and other relations on mean and covariance values. Some of these formulae require data like air density which, although not measured directly using a sonic anemometer, may be estimated with good accuracy.

These estimations may be computed at any time before their use. In SonicLib we compute them almost immediately.

\subsubsection{Pressure and dry air density}

Dry air density, necessary to compute thermal and density-related corrections, is computed according to the formula

\begin{equation}\label{eq:Dry air density}
 \rho_{d} = \frac{P_{a}}{R_{s} T_{a}}
\end{equation} 

\noindent where $P_{a}$ is site absolute pressure in Pa, $T$ the air temperature in K, and $R_{s} = 287.058$ is the specific gas constant in $\mbox{J}/(\mbox{kg}\cdot\mbox{K})$. If more customary units are used for $P_{a}$ (hPa) and for air mass (g instead of kg), then

\begin{equation}\label{eq:Dry air density expanded}
 \rho_{d} = 348.3617 \frac{P_{a}}{T}
\end{equation} 

As $T$ is expressed in K, its value can be safely replaces by sonic temperature $\theta$, itself approximating virtual temperature as shown further on. This way formula (\ref{eq:Dry air density expanded}) simplifies to

\begin{equation}\label{eq:Dry air density simplified}
 \rho_{d} = 348.3617 \frac{P_{a}}{T_{a}}
\end{equation} 

\noindent where $T_{a} = \theta + 273.15$.

Absolute pressure can not be measured using a sonic anemometer. But, it may be estimated once station height is known using hydrostatic equation

\begin{equation}\label{eq:Air pressure}
 P_{a} = 1013 \exp \left(-0.0342 \frac{z}{T} \right)
\end{equation} 

\noindent where $z$ is station altitude, and $T$ the absolute temperature in K. Using considerations similar to the ones developed for density, in SonicLib we simplify this definition as

\begin{equation}\label{eq:Air pressure simplified}
 P_{a} = 1013 \exp \left(-0.0342 \frac{z}{T_{a}} \right)
\end{equation} 

\noindent where $z$ is the station altitude above mean sea level.

\subsubsection{$\rho C_{p}$}

The product $\rho_{d} C_{p}$ is approximated in SonicLib using similar consideration on absolute and sonic temperature as used above, yielding the following formula:

\begin{equation}\label{eq:RhoCp simplified}
 \rho_{d} C_{p} = 350.25 \frac{P_{a}}{T_{a}}
\end{equation} 

Pressure is assumed estimated using formula (\ref{eq:Air pressure simplified}).

\subsection{Axis rotation}

In most real-world site the condition $\overline{w} = 0$ is not satisfied, and appropriate reference system rotation must be accomplished. This is made in SonicLib in two ways: according to the classical eddy covariance scheme (that is, on single averaging time basis) and using planar fit (on a whole campaign, or an important part of it).

\subsubsection{Classical eddy covariance}
\label{sec:Classical eddy covariance}

In classical eddy covariance axis rotation is individually accomplished on each average. In most cases two rotations are done. In special circumstances a third may be done.

\paragraph{First two rotations}
\label{sec:First two rotations}

The first two rotations are aimed at changing the reference system so that its new $x$ axis points in the direction of mean wind. The overall rotation matrix, $\mathbf{R}_{02}$, is defined as the product of two separate elementary rotations,

\begin{equation}\label{eq:First two rotations in EC}
 \mathbf{R}_{02} = \mathbf{R}_{12} \cdot \mathbf{R}_{01}
\end{equation}

\noindent where

\begin{equation}\label{eq:First rotation in EC}
 \mathbf{R}_{01} = \left[
  \begin{array}{ccc}
   \cos \alpha & \sin \alpha & 0 \\
  -\sin \alpha & \cos \alpha & 0 \\
       0       &      0      & 1
  \end{array}
 \right]
\end{equation}

\noindent is a rotation around the vertical axis with

\begin{equation}
 \sin \alpha = \frac{v}{\sqrt{u^2 + v^2}}
\end{equation} 

\noindent and

\begin{equation}
 \cos \alpha = \frac{u}{\sqrt{u^2 + v^2}}
\end{equation} 

\noindent thus aligning the transformed axis $x$ to point in the direction of horizontal wind.

The second rotation matrix complete this work, by rotating the new reference around the new $y$ axis to align the transformed $x$ axis to full three dimensional wind:

\begin{equation}\label{eq:Second rotation in EC}
 \mathbf{R}_{12} = \left[
  \begin{array}{ccc}
    \cos \beta & 0 & \sin \beta \\
         0     & 1 &      0     \\
   -\sin \beta & 0 & \cos \beta
  \end{array}
 \right]
\end{equation}

\noindent where

\begin{equation}
 \sin \beta = \frac{w_{1}}{\sqrt{u^2_{1} + w^2_{1}}}
\end{equation} 

\noindent and

\begin{equation}
 \cos \beta = \frac{u_{1}}{\sqrt{u^2_{1} + w^2_{1}}}
\end{equation} 

\noindent (here we use the symbols $u_{1}$, $v_{1}$ and $w_{1}$ to designate the three wind components in the reference obtained by applying the first rotation).

In SonicLib the two rotations are computed sequentially and applied individually to mean wind and scalar covariances (assimilable to vectors) and wind covariance matrix. The actual formulae used are:

\begin{equation}\label{eq:Transformation of wind speed in the first rotation}
  \mathbf{v}_{1}^{T} = \mathbf{R}_{01} \cdot \mathbf{v}^{T}
\end{equation} 

\begin{equation}\label{eq:Transformation of scalar covariances in the first rotation}
  \mathbf{\gamma}_{1}^{T} = \mathbf{R}_{01} \cdot \mathbf{\gamma}^{T}
\end{equation} 

\noindent and

\begin{equation}\label{eq:Transformation of wind covariances in the first rotation}
  \mathbf{C}_{1} = \mathbf{R}_{01} \cdot \mathbf{C} \cdot \mathbf{R}_{01}^{T}
\end{equation} 

\noindent where $\mathbf{v} = \left[ u,v,w \right]$ represents wind vector, $\mathbf{\gamma} = \left[ \overline{u'x'}, \overline{v'x'}, \overline{w'x'} \right]$ the ``vector'' of covariances of wind components with a generic scalar $x$, and $\mathbf{C}$ the wind covarianve matrix.

Second rotation is then applied on the result of the first, using the following relations.

\begin{equation}\label{eq:Transformation of wind speed in the second rotation}
  \mathbf{v}_{2}^{T} = \mathbf{R}_{12} \cdot \mathbf{v}_{1}^{T}
\end{equation} 

\begin{equation}\label{eq:Transformation of scalar covariances in the second rotation}
  \mathbf{\gamma}_{2}^{T} = \mathbf{R}_{12} \cdot \mathbf{\gamma}_{1}^{T}
\end{equation} 

\noindent and

\begin{equation}\label{eq:Transformation of wind covariances in the second rotation}
  \mathbf{C}_{2} = \mathbf{R}_{12} \cdot \mathbf{C}_{1} \cdot \mathbf{R}_{12}^{T}
\end{equation} 

\noindent with an intuitively extended meanings of symbols.

\paragraph{Third rotation}
\label{sec:Third rotation}

It should be easy to understand that the plane containing axes $xz$ is still perpendiculat to the horizontal plane after two rotations. This means the $w_{2}$ axis does still \emph{not} points to the the ``fluid-dynamical vertical''. For this to happen a third rotation should be made around the new $x$ axis, with matrix

\begin{equation}\label{eq:Third rotation in EC}
 \mathbf{R}_{23} = \left[
  \begin{array}{ccc}
    1 &       0      &      0      \\
    0 &  \cos \delta & \sin \delta \\
    0 & -\sin \delta & \cos \delta
  \end{array}
 \right]
\end{equation}

\noindent where

\begin{equation}\label{eq:Delta angle}
  \delta = \frac{1}{2} \arctan \left( 
    2 \frac
      {\overline{v'_{2} w'_{2}}}
      {\overline{v'_{2}v'_{2}} - \overline{w'_{2}w'_{2}}}
  \right)
\end{equation} 

Performing third rotation aligns the vertical of transformed reference to the true fluid-dynamical vertical. On the other side of this desirable fact it often happens the third rotation to be affected by error. To prevent excessive impact, thethird rotation is really executed if the rotation angle is lower than a threshold whose default value is 10 degrees, and which may be altered by the programmer as shown when documenting the call.

If third rotation angle absolute value is smaller than the threshold angle, then rotation matrix is computed and applied:

\begin{equation}\label{eq:Transformation of wind speed in the third rotation}
  \mathbf{v}_{3}^{T} = \mathbf{R}_{23} \cdot \mathbf{v}_{2}^{T}
\end{equation} 

\begin{equation}\label{eq:Transformation of scalar covariances in the third rotation}
  \mathbf{\gamma}_{3}^{T} = \mathbf{R}_{23} \cdot \mathbf{\gamma}_{2}^{T}
\end{equation} 

\noindent and

\begin{equation}\label{eq:Transformation of wind covariances in the third rotation}
  \mathbf{C}_{3} = \mathbf{R}_{23} \cdot \mathbf{C}_{2} \cdot \mathbf{R}_{23}^{T}
\end{equation} 

\subsubsection{Planar fit}

As an alternative to classical eddy covariance, in which rotations are computed individually on any averaging interval, the \emph{planar fit} method has been proposed which performs an identification of the fluid-dynamical vertical from the global behavior of many averages (typically an entire measurement campaign).

Planar fit also performs three rotations, but in a different order than classical covariance: the first two, around axes $x$ and $y$, transforms the reference so that its new vertical coincides with the fluid-dynamical vertical. The third rotation, around transformed axis $z$, aligns axis $u$ with mean wind.

Apart the different order of rotations, the most important difference between planar fit and classical rotation is that first two rotations are computed once forever from all campaign data, while the third is performed on each averaging interval.

To determine the first two rotations model

\begin{equation}\label{eq:Planar fit plane model}
  w = b_{0} + b_{1} u + b_{2} v + \varepsilon
\end{equation} 

\noindent is fitted to experimental data, assuming $\varepsilon$ to be a normally distributed random variables with zero mean and a given variance. In SonicLib fitting is determined using the standard \verb|lm| linear least squares model, assuming equal variance for $u$ and $v$ determinations.

With best fitting values $b_{0}$, $b_{1}$ and $b_{2}$ at hand, the following values are computed:

\begin{equation}\label{eq:Planar fit angle beta sine}
  \sin(\beta) = -\frac{b_{1}}{\sqrt{b^{2}_{1} + b^{2}_[2] + 1}}
\end{equation} 

\begin{equation}\label{eq:Planar fit angle beta cosine}
  \cos(\beta) = \frac{\sqrt{b^{2}_{1}+1}}{\sqrt{b^{2}_{1} + b^{2}_[2] + 1}}
\end{equation} 

\begin{equation}\label{eq:Planar fit angle gamma sine}
  \sin(\delta) = \frac{b_{2}}{\sqrt{b^{2}_{1} + 1}}
\end{equation} 

\noindent and

\begin{equation}\label{eq:Planar fit angle gamma cosine}
  \sin(\delta) = -\frac{1}{\sqrt{b^{2}_{1} + 1}}
\end{equation} 

The first two rotation matrices are then formed as

\begin{equation}\label{eq:First rotation in PF}
 \mathbf{R}_{12} = \left[
  \begin{array}{ccc}
    \cos \beta & 0 & \sin \beta   \\
        0      & 1 &      0       \\
   -\sin \beta & 0 & \cos \beta
  \end{array}
 \right]
\end{equation}

\noindent and

\begin{equation}\label{eq:Second rotation in PF}
 \mathbf{R}_{23} = \left[
  \begin{array}{ccc}
      1 &       0      &       0      \\
      0 &  \cos \delta &  \sin \delta \\
      0 & -\sin \delta &  \cos \delta
  \end{array}
 \right]
\end{equation}

The first two rotation combined matrix is then computed as

\begin{displaymath}
 \mathbf{R}_{13} = \mathbf{R}_{23} \mathbf{R}_{12}
\end{displaymath}

\noindent and applied to non-rotated data in whole set as we've seen in section \ref{sec:Classical eddy covariance}.

In planar fit, third rotation is computed individually for all data in set, with rotation matrix defined as

\begin{equation}\label{eq:Third rotation in PF}
 \mathbf{S} = \left[
  \begin{array}{ccc}
     \cos \alpha & \sin \alpha & 0 \\
    -\sin \alpha & \cos \alpha & 0 \\
          0      &      0      & 1
  \end{array}
 \right]
\end{equation}

\noindent where $\alpha = \arctan{\frac{v_{2}}{u_{2}}}$, $u_{2}$ and $v_{2}$ being the horizontal wind mean components after having applied the first two rotations. Thirs rotation is then applied the usual way.

Notice how performing the three rotations in a different order than in classical eddy covariance changes in general the three angles respect to their respective eddy covariance values, but does not modify the final result.

\subsection{Scalar fluxes and the WPL correction}

Because of the way SonicLib standard raw files are defined, all scalar concentrations are expressed as molar concentrations, that is in $\mbox{mmol}/\mbox{m}^{3}$. On return, mass densities and molar concentrations are used for fluxes, and this requires different conversions and some non identical details in how Webb-Pearman-Leuning corrections (see \cite{Webb1980}) are formulated.

SonicLib implementation follows the path described in chapters 3 and 4 of \cite{Aubinet2012}. Passages are a bit delicate, and their assessment is then made possible by storing in output data all relevant intermediate results. Advanced users may employ these as a quality assurance tool in addition to more traditional stationarity tests.

We start, by writing the expression of the fluxes $F_{s}$ and $F_{s.mol}$ of a scalar $s$ expressed in density and molar concentration form respectively. Notation is derived from \cite{Aubinet2012}, with some minor simplifications. The mass flux is

\begin{equation}\label{eq:Mass flux of a scalar with WPL correction}
 F_{s} = m_{s} \overline{w'c'_{s}} + m_{s} \frac{\overline{c_{s}}}{\overline{c_{d}}} \left[
  \left( \overline{c_{d}} + \overline{c_{v}} \right) \frac{\overline{w'\theta'}}{\overline{\theta}} +
  \overline{w'c'_{v}}
 \right]
\end{equation} 

\noindent while the molar flux is

\begin{equation}\label{eq:Molar flux of a scalar with WPL correction}
 F_{s.mol} = \overline{w'c'_{s}} + \frac{\overline{c_{s}}}{\overline{c_{d}}} \left[
  \left( \overline{c_{d}} + \overline{c_{v}} \right) \frac{\overline{w'\theta'}}{\overline{\theta}} +
  \overline{w'c'_{v}}
 \right]
\end{equation} 

As it turns out by comparing (\ref{eq:Mass flux of a scalar with WPL correction}) with (\ref{eq:Molar flux of a scalar with WPL correction}), a simple relation exists between mass and molar fluxes:

\begin{equation}
 F_{s} = m_{s} \cdot F_{s.mol}
\end{equation} 

\noindent where $m_{s}$ is the molar mass of the gas considered in units of $\mbox{g/mol}$. Table \ref{tab:Molar masses} states the molar masses of the gases known to SonicLib.

\begin{table}
\centering
 \begin{tabular}{cc}
  \hline
  Gas / Element & Molar mass $m_{s}$ in  $\mbox{g/mol}$ \\
  \hline
  Water          & 18.0153 \\
  Carbon dioxide & 44.0100 \\
  Ammonia        & 17.0305 \\
  Methane        & 16.0425 \\
  \hline
  Hydrogen   &  1.00794 \\
  Oxygen     & 15.9994 \\
  Carbon     & 12.0107 \\
  Nitrogen   & 14.0067 \\
  \hline
 \end{tabular} 
\caption{Molar mass of gases known to SonicLib and their component chemical elements}
\label{tab:Molar masses}
\end{table} 

\subsection{Sensible heat fluxes $H_{0v}$ and $H_{0}$}\label{sub:H0}

It is well known that sonic temperature $\theta$ is connected to dry bulb temperature $T$ by relation (see \cite{Sozzi2002} and \cite{Kaimal1991})

\begin{equation}\label{eq:Sonic temperature}
 \theta = T \cdot \left( 1 + 0.51 q \right)
\end{equation} 

\noindent and a very similar relation to hold for virtual temperature,

\begin{equation}\label{eq:Virtual temperature}
 \theta_{v} = T \cdot \left( 1 + 0.61 q \right)
\end{equation} 

\noindent where $q$ represents the mass mixing ratio of water to air.

Taking the difference between these temperatures we get

\begin{equation}\label{eq:Difference between virtual anc sonic temperatures}
 \theta_{v} - \theta = 0.1 Tq
\end{equation} 

\noindent whose value is very small. As a consequence, in sonic anemometry it is commonly assumed that $\theta = \theta_{v}$.

If this is accepted for true, then the same will happen to any quantity containing sonic temperature, namely the covariance $\overline{w'\theta'}$, which may be safely interpreted as $\overline{w'\theta'_{v}}$. This covariance, multiplied by $\rho C_{p}$, yields the \emph{surface buoyancy flux} (see \cite{Garratt1992}):

\begin{equation}\label{eq:Surface buoyancy flux}
 H_{0v} = \rho_{d} C_{p} \overline{w'\theta'_{v}}
\end{equation} 

If all we have is a sonic anemometer we can say no more, and use (\ref{eq:Surface buoyancy flux}) as an approximation to the actual \emph{sensible heat flux}

\begin{equation}\label{eq:Sensible heat flux}
 H_{0} = \rho C_{p} \overline{w'T'}
\end{equation} 

If we have water in our data set, however, a correction may be used (see \cite{Schotanus1983} and \cite{Sozzi2002}) in the form

\begin{equation}\label{eq:Schotanus correction}
 \overline{w'T'} = \overline{w'\theta'} - 
    0.51 \overline{T} \cdot \overline{w'\chi'} + 
    2 \frac{\overline{T} \overline{u}}{\overline{c^{2}}} \overline{u'w'}
\end{equation} 

\noindent where $\chi$ is water mass mixing ratio. In equation \ref{eq:Schotanus correction} $T$ represents an absolute temperature, whose numerical value may be safely replaced by $T_{a} = \theta + 273.15$.

As pointed out in \cite{Foken2008} the term $2 \frac{\overline{u}}{403} \overline{u'w'}$ only applies to vertically directed measuring path, which is not always the case with 3D ultrasonic anemometers. Variations have been proposed for this term taking into account the full geometry of instrument, with different constants used in different sonic anemometers. Most often on these days the revised term is inforporated in sonic anemometer firmware, and needs not to be applied. This given, in SonicLib the following simplified relation is used: 

\begin{equation}\label{eq:Schotanus correction simplified}
 \overline{w'T'} = \overline{w'\theta'} - 
    0.51 \overline{\theta} \cdot \overline{w'\chi'}
\end{equation} 

\subsection{Latent heat flux $H_{e}$}

The latent heat flux is computed accordingly to \cite{Sozzi2002} as

\begin{equation}\label{eq:Latent heat flux}
 H_{e} = \frac{\lambda}{1000} \overline{w'q'}
\end{equation} 

\noindent where $\overline{w'q'}$ is expressed in $\mbox{g}/\mbox{m}^2 \mbox{s}$ and

\begin{equation}\label{eq:Latent condensation heat}
 \lambda = 2500.8 - 2.36 \theta + 0.0016 \theta^2 - 0.00006 \theta^3
\end{equation} 

\noindent is latent condensation heat (note $\theta$ is expressed in Celsius degrees, not K).

\subsection{Examples}
\subsubsection{Classical eddy covariance ``the normal way''}

The first step consists in a quite standard processing, the kind one most often does, aimed at getting results in the shortest possible time.

In this example we use the ``CRA.01'' data set (\ref{sec:CRA.01}). But instead of working on it directly in SonicLib standard raw form, we begin from start: a nice way to demonstrate an entire workflow. As shown in section \ref{sec:CRA.01} the original data have been collected using a special purpose data acquisition system, written using National Instrument's LabVIEW and operating on an industrial PC.

The resulting files bear very little resemblance to SonicLib files, and must then be converted. This is accomplished outside SonicLib using specially designed ``adapter'' software.

The actual conversion is quite involved, reflecting the complexity of the original format. This originates from the data acquisition system, which on each read encodes textual data from sonic anemometer to binary form. Data are then saved, but as the system has been written in LabVIEW they are not stored as (say) regular Intel binary numbers. They are stored as LabVIEW binary instead, and this turns out a big difference: LabVIEW binary integers are encoded in big-endian form (most significant byte first in memory) while Intel binary numbers are encoded in little-endian form (least significant byte first in memory). 

Converting between the two forms is straightforward (it is sufficient to swap bytes in any number), yet needs to be taken care of, and this in turn demands using a few low-level operations.

The first conversion step is performed using program \verb|decode|, also released as public domain thanks to Servizi Territorio srl, partner of SonicLib team, who wrote it (you may find the source in CRA.01 data set directory).

Use of \verb|decode| is simple, once some basic knowledge of data collection text is known. The program is run from the command line as

\begin{verbatim}
decode <input> <type> <output>
\end{verbatim}

\noindent where \verb|<input>| designates the input file, \verb|<output>| the output file name, and \verb|<type>| an integer number specifying the file type. And this is precisely the first thing to know: what the type is.

The types supported by the \verb|decode| utility are:

\begin{description}
 \item[1] Meteoflux V3.x format (a binary format use on an old data acquisition system made by Servizi Territorio srl).
 \item[2] LabVIEW-based data acquisition system, with 32 bit integers used to encode sonic raw data (aka ``LabVIEW long'').
 \item[3] LabVIEW-based data acquisition system, with 16 bit integers used to encode sonic raw data (aka ``LabVIEW short'').
\end{description}

Original data in CRA.01 set are in LabVIEW short form. How do we know it? The ideal answer would have been ``Oh, because thanks to a rigorous configuration management process we know exactly which format the data are, for any specific file ever collected.''

But this is not the case! Data acquisition systems have been modified quite often, to follow the quick evolution of sonic anemometers. The result is a huge set of campaign data collected in many different places with different systems. But fortunately, there is a simple method to say with absolute certainty which form files are, by just ``looking at file size''. We know each CRA.01 file is 721 kByte long, and that all LabVIEW short files are exactly 721 KByte each. This means CRA.01 original data are in LabVIEW short form.

When \verb|decode| is launched, it produces an output file which still is not in SonicLib form, but in Meteoflux Core text form. This is still another proprietary format by Servizi Territorio srl, but with a decisive advantage over LabVIEW short form: it's human readable! And, recent enough to devise its own adapter, not in SonicLib library but anyway as part of the SonicLib project: \verb|mfc1|.

Similarly to \verb|decode|, \verb|mfc1| operates as a command-line utility. Torun it, just type

\begin{verbatim}
mfc1 <input> [<cvt>] <output>
\end{verbatim}

\noindent where

\begin{description}
 \item[<input>] designates an input file in Meteoflux Core textual form (as produced by \verb|decode|).
 \item[<cvt>] is a conversion file (its being surrounded by square brackets indicates it's \emph{optional}). More on later: it's a very important file. If omitted, no conversion is performed on analog data, if any.
 \item[<output>] finally designates an output file, this time in SonicLib form.
\end{description}

All interesting behavior of \verb|mfc1| is specified by the contents of the <cvt> file. This is a text file containing, for each of the analog columns to convert, the following information:

\begin{description}
 \item[Channel] Channel number to which the instrument provoding the data has been physically connected (1 to 14 for Meteoflux Core using Metek USA-1 anemometer).
 \item[Multiplier] Coefficient $m$ in conversion relation $m \cdot x + q$.
 \item[Offset] Term $q$ in conversion relation $m \cdot x + q$.
 \item[Meanings] A string, containing the short name of the quantity gathered. The possible values are given in table \ref{tab:Meanings values in conversion}.
\end{description}

\begin{table}
 \begin{tabular}{cc}
  \hline
  Value of ``Meanings'' & Actual meanings, in plain form \\
  \hline
           q            & Water, in $\mbox{mmol}/\mbox{m}^3$ \\
           c            & Carbon dioxide, in $\mbox{mmol}/\mbox{m}^3$ \\
           a            & Ammonia, in $\mbox{mmol}/\mbox{m}^3$ \\
           m            & Methane, in $\mbox{mmol}/\mbox{m}^3$ \\
           temp         & Air temperature, in Celsius degrees \\
           hrel         & Relative humidity, in \% \\
  \hline
 \end{tabular} 
\caption{Values of ``Meanings'' field known to SonicLibto date}
\label{tab:Meanings values in conversion}
\end{table} 

Filling in the details of conversion file demands deep knowledge of the ultrasonic anemometer, the other sensors connected, and collection process. Nothing really difficult: just something to be done with care.

In the specific case CRA.01 data set instrument configuration and conversion data have been described extensively in section \ref{sec:CRA.01 instrument configuration}. Here we limit to use the values there computed:

\begin{description}
 \item[$\mbox{H}_{2}\mbox{O}$] Multiplier = 0.0610370, Offset = 0.
 \item[$\mbox{C}\mbox{O}_{2}$] Multiplier = 0.000610370, Offset = 10.
\end{description} 

Since water and carbon dioxide channels from the LI-7500 have been connected to analog inputs no. 7 and 8 of USA-1, the following concents may be used for the conversion file:

\begin{verbatim}
7 0.0610370 0.0 "q"
8 0.000610370 10.0 "c"
\end{verbatim}

Using \verb|decode| and \verb|mfc1| may be tedious, as both expect users to specify single files. They may be invoked however from suitable scripts (e.g. in Python) to automate file production within a directory. These script may well be ad-hoc and very small, or as complex as full-fledged software packages. One of such automation scripts is given here as just an example - and a sub-optimal one indeed:

\begin{verbatim}
#!/usr/bin/python

import glob
import sys
import os

files = glob.glob("*.*r")

for file in files:
	out = file[0:len(file)-1]+".csv"
	os.system("./mfc1 "+file+" cra01.cfg "+out)
\end{verbatim} 

This script converts automatically all files with names terminating with one character 'r' (that is, a lowercase r) to SonicLib raw data files in the same directory. Once execution has terminated, SonicLib standard files for raw data may be copied manually to data source directory, in our case \verb|<SonicLib_Root>/TestData/CRA.01|.

Up to now, we have performed no more than conversion of data: maybe a little tedious task, yet simple after all. Really interesting things begin here.

The very first step consists in translating the huge mass of SonicLib files to averages and covariances, in preparation to eddy covariance. To perform this step we may use a function we've met already, \verb|average.sonic.file.set|, with a call like this:

\begin{verbatim}
d <- average.sonic.file.set(
  dir.name="../TestData/CRA.01",
  verbose=TRUE
)
\end{verbatim} 

As you see, we in practice accepted all defaults. Part of them are already consistent with data as they are (like for example data rate). Others, namely the averaging period, have a default which accidentally concides with the one we want for our case: 30 minutes. A special mention goes to delay, which we left untouched: this is a good thing, the 0.3s default being consistent with how the Li-7500 sensor has been programmed on field.

Specifying \verb|verbose=TRUE| enables printing status and error messages, and may be helpful for an interactive run like the one we're running in this example. Within an automated processing procedure, specifying \verb|verbose=TRUE| may be quite annoying, and leaving the default might be more acceptable. No fixed rule exists, however, and anyone may proceed as their taste suggests.

At end of processing we have in our hands a new object, \verb|d|, of type \verb|sonic.avg.data|, \emph{or} of value \verb|NULL| if something goes awry. From the description given in section \ref{sec:Average Multiple Raw Data Files} we know this \emph{may} happen, and a check is then worth.

In line of principle, verifying if a value is NULL without printing the whole of it (which may be unreasonably large) may be done in a very simple way:

\begin{verbatim}
is.null(d)
\end{verbatim}

The result is a boolean value, TRUE or FALSE depending on the answer. Better even, we may give a command like

\begin{verbatim}
summary(d$data)
\end{verbatim}

\noindent which in case of a NULL value yields just the NULL, but upon successful return gives a wealth of interesting statistics about the results obtained.

In our case execution has completed successfully, so \verb|d| is really not NULL. As a very simple test we just plot the average wind speed, sonic temperature, water and carbon dioxide. Results, shown in figures ref{fig:CRA.01 Vel}, \ref{fig:CRA.01 Temp}, \ref{fig:CRA.01 Q} and \ref{fig:CRA.01 C}, are quite heartwarming: as small as the CRA.01 sample is (1 day), plots show some dynamics: whatever we've done, instruments were not completely dead.

\begin{figure}[htp]
 \centering
 \begin{center}
 \includegraphics[scale=1.1,keepaspectratio=true]{./diagrams/CRA01_Vel.pdf}
 % get.raw.data.1.pdf: 288x180 pixel, 72dpi, 10.16x6.35 cm, bb=0 0 288 180
 \end{center}
 \caption{Evolution of wind speed in CRA.01 data set}
 \label{fig:CRA.01 Vel}
\end{figure}

\begin{figure}[htp]
 \centering
 \begin{center}
 \includegraphics[scale=1.1,keepaspectratio=true]{./diagrams/CRA01_Temp.pdf}
 % get.raw.data.1.pdf: 288x180 pixel, 72dpi, 10.16x6.35 cm, bb=0 0 288 180
 \end{center}
 \caption{Evolution of temperature in CRA.01 data set}
 \label{fig:CRA.01 Temp}
\end{figure}

\begin{figure}[htp]
 \centering
 \begin{center}
 \includegraphics[scale=1.1,keepaspectratio=true]{./diagrams/CRA01_Q.pdf}
 % get.raw.data.1.pdf: 288x180 pixel, 72dpi, 10.16x6.35 cm, bb=0 0 288 180
 \end{center}
 \caption{Evolution of water molar concentration in CRA.01 data set}
 \label{fig:CRA.01 Q}
\end{figure}

\begin{figure}[htp]
 \centering
 \begin{center}
 \includegraphics[scale=1.1,keepaspectratio=true]{./diagrams/CRA01_C.pdf}
 % get.raw.data.1.pdf: 288x180 pixel, 72dpi, 10.16x6.35 cm, bb=0 0 288 180
 \end{center}
 \caption{Evolution of carbon dioxide molar concentration in CRA.01 data set}
 \label{fig:CRA.01 C}
\end{figure}

The fact we have seen a dynamics of course does not mean data are correct, nor it implies they are suitable for eddy covariance (maybe because of important non-stationarity). We'll be back on these important issues, but for the moment we proceed contenting ourselves with minimal evidence of quality.

The next step is, finally, eddy covariance. The most important decision we have to take at this point is its \emph{type}. The data set is so short we can immediately rule planar fit out. But a choice is still possible between the two flavors of classical eddy covariance: with two, or three rotations?

To take this choice we may proceed in many ways. Two popular methods are:

\begin{itemize}
 \item Try 2 and 3 rotations, and use the better looking result set. This way demands a method exists to decide which result set ``looks better'', and once again many methods exist. A sensible one is to check key correlations arising from similarity and say the best result set is the one for which they get the best score.
 \item Use a priori knowledge and select the most appropriate type. For example in CRA.01 data set the flat experimental field suggests limiting to 2 rotations.
\end{itemize}

Notice some research groups do not recommend using 3 rotations (see for example (\cite{Aubinet2012})).

In this example we adopt two rotations, which incidentally is the default value. This important decision taken, we may finally run eddy covariance:

\begin{verbatim}
e <- eddy.covariance(
  d,
  station.altitude=20
)
\end{verbatim}

We may now have a look to what we obtained.

\begin{figure}[htp]
 \centering
 \begin{center}
 \includegraphics[scale=1.1,keepaspectratio=true]{./diagrams/CRA01_H0v.pdf}
 % get.raw.data.1.pdf: 288x180 pixel, 72dpi, 10.16x6.35 cm, bb=0 0 288 180
 \end{center}
 \caption{Evolution of buoyancy flux (aka ``sensible heat flux'' when a fast water sensor is missing) in CRA.01 data set}
 \label{fig:CRA.01 H0v}
\end{figure}

In figure \ref{fig:CRA.01 H0v} we may see the buoyancy flux. This is a very interesting indicator, yielding a first impression of sensible heat flux even if a fast water sensor is missing. It is negative on first (``leftmost'') night-time, and positive during daytime, as normally happens under daytime convective conditions. On second night, however, things change dramatically, with buoyancy flux moving just around the zero reference line. What happened?

Trying to reconstruct past meteorological events from a single variable is not more serious than figuring out an entire animal given a single tooth, 'a la Baron Cuvier. This is the reason field campaigns are accompanied by extensive meteorological observation data. Yet \emph{something} may be conjectured, and results may sometimes be interesting.

In our case, a well known experimental fact is that sensible heat flux (and buoyancy flux) visually ``copy'' the time evolution of solar net radiation. On a sunny day, at an ideal rural site not affected by spurious shading, the net radiation varies smoothly from a small negative value on nighttime to a larger positive value on daytime, the maximum being reached at local noon.

Under identical conditions the same would happen to buoyancy flux. But figure \ref{fig:CRA.01 H0v} tells us an entirely different story: especially on daytime we have a distinct sawtooth pattern in the graph, which leads us to imagine clouds intercepting a lot of incoming shortwave radiation. If precipitation also occurred, the terrain has poor draining properties (or was already saturated by a preceding rain episode, or is waterlogged, or \ldots) and rain was intense enough to saturate the ground, then we'd likely get a very small night-time ground cooling due to the large thermal capacity of water, and consequently the sensible heat flux would drop on nighttime. And the same would happen to buoyancy flux, a variant of it.

\begin{figure}[htp]
 \centering
 \begin{center}
 \includegraphics[scale=1.1,keepaspectratio=true]{./diagrams/CRA01_H0.pdf}
 % get.raw.data.1.pdf: 288x180 pixel, 72dpi, 10.16x6.35 cm, bb=0 0 288 180
 \end{center}
 \caption{Evolution of sensible heat flux (in black) in CRA.01 data set; the pink continuous line shows, for comparison, the buoyancy flux}
 \label{fig:CRA.01 H0}
\end{figure}

Figure \ref{fig:CRA.01 H0} represents the sensible heat flux after correction of density and water flux effects have been removed. We may see changes from buoyancy flux are quite small, and evident only on daytime. On the ``leftmost'' nighttime a bit of variation is also visible, but on the rightmost the two fluxes are virtually identical.

\begin{figure}[htp]
 \centering
 \begin{center}
 \includegraphics[scale=1.1,keepaspectratio=true]{./diagrams/CRA01_He.pdf}
 % get.raw.data.1.pdf: 288x180 pixel, 72dpi, 10.16x6.35 cm, bb=0 0 288 180
 \end{center}
 \caption{Evolution of latent heat flux in CRA.01 data set}
 \label{fig:CRA.01 He}
\end{figure}

Figure \ref{fig:CRA.01 He} shows the \emph{latent} heat flux. This graph is very interesting, since what we have already conjectured. On first nighttime and during daytime its value was always positive. On the second, rightmost, nighttime, instead, the sensible heat flux drops to zero.

\begin{figure}[htp]
 \centering
 \begin{center}
 \includegraphics[scale=1.1,keepaspectratio=true]{./diagrams/CRA01_Fq.pdf}
 % get.raw.data.1.pdf: 288x180 pixel, 72dpi, 10.16x6.35 cm, bb=0 0 288 180
 \end{center}
 \caption{Evolution of water molar flux in CRA.01 data set}
 \label{fig:CRA.01 Fq}
\end{figure}

Sensible heat flux tightly connects with the flux of water vapor, whose time evolution we see in figure \ref{fig:CRA.01 Fq}. Both quantities indicate a massive peak afternoon, as if precipitation has fallen on a warm ground, with consequent large evaporation. Has relative humidity then grown to impede further evaporation on the following nighttime? Did ground saturate of water, increasing hugely its thermal capacity?

\subsubsection{A bit of sane skepticism}\label{sec:Sane skepticism}

The questions closing the preceding sections may remain unanswered, if no meteorological observations are done during a field campaign. This might be a pity, especially in ecology and agricolture, as it makes quite difficult to separate purely physical effects (e.g. due to regolith warming and cooling in absence of vegetation) and the response of a forest camopy or crop.

But at least we might resort to some detective work the kind we've seen. We can gain conjectures, not experimental proofs, but at least we have something.

The question then arises, whether what we have is really there, or not.

Eddy covariance in fact is guaranteed to give sensible results only under quite restrictive conditions, stationarity at least up to second order being one. In many cases (as often happens for example with fixed meteorological stations aimed at feeding input data to pollutant dispersion models) we can't be that finicky with data quality, and even values obtained when the assumptions behind eddy covariance are not met are considered better than gaps filled with an act of creativity.

But if a possibility of check is given, it is good practice using it.

So, what may we say of CRA.01 set? Are eddy covariance assumptions met? More importantly, \emph{how} may we check?

As a starter, we check data against non-stationarity - one of the most common cases of eddy covariance error.

In view of this objective we proceed the classical way, by locating data items for which relative non-stationarity of wind components and non-steadiness of covariances fall beyond their associated limits (see \cite{Aubinet2012}).

Within SonicLib, relative non-stationarity and non-steadiness are computed before rotations, so that non-stationarity from any wind component may contribute to non-stationarity along rotated vertical. For sake of brevity, and just for this example, we will restrict however to non-rotated vertical indicators.

We begin with relative non-stationarity of vertical wind, shown in figure \ref{fig:CRA.01 Rel Nonstat W} along with a reference line corresponding to the threshold, 0.50.

\begin{figure}[htp]
 \centering
 \begin{center}
 \includegraphics[scale=1.1,keepaspectratio=true]{./diagrams/CRA01_W_RelNon.pdf}
 % get.raw.data.1.pdf: 288x180 pixel, 72dpi, 10.16x6.35 cm, bb=0 0 288 180
 \end{center}
 \caption{Evolution of relative non-stationarity of vertical wind in non-rotated frame in CRA.01 data set}
 \label{fig:CRA.01 Rel Nonstat W}
\end{figure}

Macroscopic anomalies apart, we may notice that 18 out of 48 data items exceed the threshold for relative non-stationarity of vertical wind. This number, apparently large (37.5 \% of the whole data set) is somewhat in the middle of the values observed for ocean surface (15 \%) and forests (55 \%) (see \cite{Aubinet2012}). Overall, this looks not bad (although the data set is so small we can't honestly draw any statistically sound truth). Yet there are 18 values for which relative non-stationarity is significantly large. What should we do on them?

Before attempting to devise an answer, let's have a look on non-steadinesses. In principle we should chech the ones from all possible covariances, but as before we limit attention to the three we're immediately interested to: $\overline{w'\theta'}$, $\overline{w'q'}$ and $\overline{w'c'}$.

We can see them in figures \ref{fig:CRA.01 Nonste WT}, \ref{fig:CRA.01 Nonste WQ} and \ref{fig:CRA.01 Nonste WC}.

\begin{figure}[htp]
 \centering
 \begin{center}
 \includegraphics[scale=1.1,keepaspectratio=true]{./diagrams/CRA01_WT_NonSte.pdf}
 % get.raw.data.1.pdf: 288x180 pixel, 72dpi, 10.16x6.35 cm, bb=0 0 288 180
 \end{center}
 \caption{Evolution of non-steadiness of $\overline{w'\theta'}$ in CRA.01 data set}
 \label{fig:CRA.01 Nonste WT}
\end{figure}

The typical threshold for non-steadiness is 0.30 (see \cite{Aubinet2012}). As we had made with vertical wind relative non-stationarity, figures \ref{fig:CRA.01 Nonste WT}, \ref{fig:CRA.01 Nonste WQ} and \ref{fig:CRA.01 Nonste WC} show a pink reference line at threshold level, allowing to see when exceedances have occurred.

\begin{figure}[htp]
 \centering
 \begin{center}
 \includegraphics[scale=1.1,keepaspectratio=true]{./diagrams/CRA01_WQ_NonSte.pdf}
 % get.raw.data.1.pdf: 288x180 pixel, 72dpi, 10.16x6.35 cm, bb=0 0 288 180
 \end{center}
 \caption{Evolution of non-steadiness of $\overline{w'q'}$ in CRA.01 data set}
 \label{fig:CRA.01 Nonste WQ}
\end{figure}

\begin{figure}[htp]
 \centering
 \begin{center}
 \includegraphics[scale=1.1,keepaspectratio=true]{./diagrams/CRA01_WC_NonSte.pdf}
 % get.raw.data.1.pdf: 288x180 pixel, 72dpi, 10.16x6.35 cm, bb=0 0 288 180
 \end{center}
 \caption{Evolution of non-steadiness of $\overline{w'c'}$ in CRA.01 data set}
 \label{fig:CRA.01 Nonste WC}
\end{figure}

With threshold exceedance rates of 0.146, 0.229 and 0.229 respectively for flux of temperature, water and carbon dioxide, the situation pertaining non-steadiness shows an incidence not far from the 0.375 we've seen for vertical wind relative non-stationarity. Distribution of exceedances in time is, however, quite different. A method for summarizing our findings is then welcome.

A sensible approach is to attribute each average a \emph{quality meter}. Or, if you prefer, a \emph{problem index}, and this is the way we do it.

Suppose each time we find some exceedance (of relative non-stationarity of $w$, or of any one of the three non-steadiness values) we increment a ``trouble counter''. The end result will be associating to each averaging block an integer number ranging from 0 (trouble-free, ideal situation) to 4 (all non stationarity indicators exceeded their threshold, the worst possible case).

This is very easy to accomplish in R. First of all, we create a vector of numbers of the same length of the number of averaging blocks:

\begin{verbatim}
W.rns <- e$data$w.rns
trbl <- numeric(length(W.rns))
\end{verbatim}

This first line just extracted vertical wind relarive non-stationarity, just to save typing. The important line is in reality the second one, creating a vector of the right length all whose elements have the value 0 (you may test this).

We may then increment the counter for the averaging blocks where vertical wind relative non-stationarity exceeded the threshold 0.5:

\begin{verbatim}
idx <- abs(W.rns) > 0.5
trbl[idx] <- trbl[idx] + 1
\end{verbatim}

The first line creates an index vector \verb|idx| of boolean values, where the TRUEs indicate an exceedance. This index vector is used in the second line to increment all components of trouble vector \verb|trbl| where an exceedance has occurred.

We may then repeat the process using this time the three non-steadinesses, and incrementing exactly where they occur:

\begin{verbatim}
WT.nst <- e$data$wt.nst
idx <- abs(WT.nst) > 0.3
trbl[idx] <- trbl[idx] + 1
WQ.nst <- e$data$wq.nst
idx <- abs(WQ.nst) > 0.3
trbl[idx] <- trbl[idx] + 1
WC.nst <- e$data$wc.nst
idx <- abs(WC.nst) > 0.3
trbl[idx] <- trbl[idx] + 1
\end{verbatim}

If we type \verb|trbl| from the R command prompt we can see how many troubles we have, and when.

\begin{verbatim}
 [1] 0 0 1 0 0 0 1 0
 [9] 0 1 0 0 2 2 1 0
[17] 0 0 0 0 0 0 0 0
[25] 1 0 0 0 3 3 3 2
[33] 2 1 4 0 2 2 4 3
[41] 2 3 0 2 0 0 0 2
\end{verbatim}

It is interesting to note that many averaging blocks are trouble-free. A couple, however, totalized 4 troubles each, indicating possible problems with the averages.

If the troubles score shows such a moved and interesting dynamics, how might we use it? The answer, non unique, depends on the context and the application.

A possible line of action might be to not consider in final evaluations all data whose trouble count exceeds some value. Or, troubled data may be flagged as suspect, and their users warned of possible problems would they use flagged data (discharging in the end any responsibility to their judgement). These are just two example, of the many possible we may find.

On a more practical side we pose another problem: are non-stationarity indicators \emph{enough} to pinpoint problems in data? Maybe not? We will see a possible glimpse in the next section.

\subsubsection{Non-stationarity, rotation angles and very strange results}

One of the SonicLib developers has witnessed a very strange output from a few fixed eddy covariance installations, where occasionally the buoyancy flux exceeds by half an order of magnitude the incoming shortwave radiation.

This nuisance is not frequent (six cases per year in the worse affected station), and its manifestation is macroscopic enough to not survive validation. Yet it's annoying, and provisions were taken to perform a specific investigation.

The result was very interesting: the strange cases occurred when wind was subject to a sudden change in strength or direction, accompanied by significant change of temperature. These conditions are not impossible in Po Valley, especially associated with Foehn development.

These sudden velocity changes, acting as macroscopic non-stationarities, interfered with the determination of rotation angles. Of these, the first occurs around the vertical axis and its consequences are typically harmless.

But this is not true which the \emph{second} rotation. It may in fact happen a very large tilt is applied, due to the massive non-stationarity, while terrain and good sense would have suggested a near-horizontal wind tangent plane.

When the incorrectly large tilt rotation is applied, the large covariance between horizontal wind components and changing temperature leaks into the vertical direction, and is counted by eddy covariance as an unrealistically huge vertical buoyancy flux.

It is worth mentioning this effect occurs only if non-stationarity on wind velocity is accompanied in synch by a nonstationarity in temperature. If only one of these conditions is met important errors also occur, but the magnitude of the change to buoyancy flux is not sufficient to allow easy detection on visual inspection.

On a deep level this real-world example shows how catastrophic effects may come from a wrong partition between mean and turbulent parts of natural signals. The issue is problematic, however, as the mean and turbulent partitioning schemes in use to date are based in most cases on time averages over a fixed length interval, whose limits are not connected to abrupt changes in the signal. Solution of this problem may challenge even adaptive averaging interval length determination, and maybe will involve some redefinition of partitioning scheme on more physical grounds.

On the more practical side, we maybe have just been given the gift of another data quality indicator: value of second rotation angle!

That this can eventually be can be visualized by plotting the second rotation angle as a function of time. We've made on CRA.01 data set, and results are presented in figure \ref{fig:CRA.01 Second Angle}.

\begin{figure}[htp]
 \centering
 \begin{center}
 \includegraphics[scale=1.1,keepaspectratio=true]{./diagrams/CRA01_Rot2.pdf}
 % get.raw.data.1.pdf: 288x180 pixel, 72dpi, 10.16x6.35 cm, bb=0 0 288 180
 \end{center}
 \caption{Evolution of second rotation angle in CRA.01 data set}
 \label{fig:CRA.01 Second Angle}
\end{figure}

As flat as the CRA.01 site is locally, second rotation angle assumes sometimes veri large values. In addition if we look at sign we see it's always negative, indicating for CRA.01 data a systematic vertical component.

Second rotation angles for CRA.01 set, by the way, are much smaller than observed occasionally at the faulty location, where values in excess of 30 degrees have been observed. On the basis of second rotation angle alone we have no apparent reason to label some of the 30 minutes averages in CRA.01 suspect.

Yet, might this check devise some further investigation?

\subsubsection{Removing linear trends}

One of the many forms nonstationarity may take is a gentle monotonically increasing or decreasing ``trend''.

In many cases this ``trend'' may be well approximated by straight segments, which can be approximated from data.

If outliers are not exceedingly frequent, a sensible (in the senses: economical and implementable in one line of code within R) way is using linear regression, as explained in section \ref{sec:sonic.avg.data}.

In a more SonicLib flavor, we may enable trend removal by specifying \verb|trend.removal| to something different from the default \verb|"none"| in calls to \verb|average.sonic.data| and \verb|average.sonic.file.set|.

The following example shows processing of data set CRA.01 with and without trend removal, and producing a plot of buoyancy flux.

\begin{verbatim}
# Compute control and treatment
d0<-average.sonic.file.set(
  "../TestData/CRA.01",
  trend.removal="none",
  verbose=TRUE);
d1<-average.sonic.file.set(
  "../TestData/CRA.01",
  trend.removal="linear",
  verbose=TRUE);
ec0 <- eddy.covariance(d0, station.altitude=50);
ec1 <- eddy.covariance(d1, station.altitude=50);
t.stamp <- d0$data$t.stamp;
H0.v.trend   <- ec0$data$H0.v;
H0.v.detrend <- ec1$data$H0.v;

# Plot buoyancy fluxes with and without trend removal
pdf(file="../manual/diagrams/H0v_detrend.pdf",
  width = 4.2, height = 3.5);
plot(t.stamp, H0.v.trend,
  type="l", xlab="",
  ylab="Buoyancy flux (W/m2)",
  col="red");
lines(t.stamp, H0.v.detrend,
  col="blue");
lines(t.stamp, rep(0,times=48),
  col="black");
dev.off();
\end{verbatim}

(Incidentally, you may note this example may be run as an automatic procedure).

\begin{figure}[htp]
 \centering
 \begin{center}
 \includegraphics[scale=1.1,keepaspectratio=true]{./diagrams/H0vDetrend.pdf}
 % get.raw.data.1.pdf: 288x180 pixel, 72dpi, 10.16x6.35 cm, bb=0 0 288 180
 \end{center}
 \caption{Buoyancy flux with (blue) and without (red) linear trend removal}
 \label{fig:H0v detrend}
\end{figure}

The result of preceding example can be seen in figure \ref{fig:H0v detrend}. In this specific case you may see how the two curves are similar, a visual suggestion any trend present in signal was not severe enough to perturb the covariance $\overline{w'\theta'}$.

This is not necessarily the case, however. Cases exist in which important indicators change significantly, at least if we consider visual impact. Linear detrending may then improve final results.

Is this always true? In general the answer is negative: it depends on which shape the actual trend has!

As a counterexample, imagine a data series of length 36000 representing (say) temperature. The series is composed by a perfectly constant block of length 18000, of value 20, followed by 18000 data items whose value increases linearly from 20 to 22. This data item may be generated using the R command

\begin{verbatim}
tt <- c(rep(0,times=18000),
  seq(from=20,to=22,length.out=18000))
\end{verbatim}

We may then compute the regression line,

\begin{verbatim}
tau<-1:36000
tt.l<-lm(tt~tau)
\end{verbatim}

\noindent whose corresponding equation is

\begin{equation}
 5.556 \cdot 10^{-5} \cdot t + 19.50
\end{equation}

The problem in this case is we're trying to approximate a piecewise-linear function with a (simpler) straight line. The deviation of trend from function is quite apparent in figure \ref{fig:Trend Example}.

\begin{figure}[htp]
 \centering
 \begin{center}
 \includegraphics[scale=1.1,keepaspectratio=true]{./diagrams/TrendExample.pdf}
 % get.raw.data.1.pdf: 288x180 pixel, 72dpi, 10.16x6.35 cm, bb=0 0 288 180
 \end{center}
 \caption{Example of an artificial signal (in blue), simulating temperature, and (in red) its fitted linear trend}
 \label{fig:Trend Example}
\end{figure}

Residuals from trend in a case as depicted in figure \ref{fig:Trend Example} are large enough to elicit immediate suspect. But what happens on real-world signals, especially ``convective'', where variation may be more apparent than trend?

Checking some more quantitative aspect may prove helpful in these cases. For example we may try to compare relative non stationarity (of $w$ in our case) or non-steadinss (for $\overline{w'\theta'}$).

Figures \ref{fig:Rns detrend} and \ref{fig:Nst detrend} show what happens in CRA.01 case.

\begin{figure}[htp]
 \centering
 \begin{center}
 \includegraphics[scale=1.1,keepaspectratio=true]{./diagrams/WRnsDetrend.pdf}
 % get.raw.data.1.pdf: 288x180 pixel, 72dpi, 10.16x6.35 cm, bb=0 0 288 180
 \end{center}
 \caption{Relative non stationarity for wind component $w$ with (blue) and without (red) linear trend removal}
 \label{fig:Rns detrend}
\end{figure}

In figure \ref{fig:Rns detrend} we can see only one line of the expected two. This is because relative non-stationarity for wind component $w$ is very close to 0 and covered by the reference black line. But this adds very little to our knowledge: by the very way it's defined (linear regression of experimental data using time as independent variable) relative non-stationarity ``must'' drop to 0 (within roundoff errors).

More interesting is the behavior of non-steadiness, whose connection with linear trend removal is less strong. As figure \ref{fig:Nst detrend} suggests, some decrease occurs especially in correspondence of the two large peaks at 6 and 19. The smaller peak at 17 o'clock has remained perfectly unchanged however.

\begin{figure}[htp]
 \centering
 \begin{center}
 \includegraphics[scale=1.1,keepaspectratio=true]{./diagrams/WTNstDetrend.pdf}
 % get.raw.data.1.pdf: 288x180 pixel, 72dpi, 10.16x6.35 cm, bb=0 0 288 180
 \end{center}
 \caption{Non steadiness of $\overline{w'\theta'}$ with (blue) and without (red) linear trend removal}
 \label{fig:Nst detrend}
\end{figure}

The effect of detrending may then be beneficial in many cases, but we should take for granted specific situations will present where this strategy is not ideal. This is not surprising: nature is \emph{non}-linear, and imposing linearity on it is nothing more than a convenient artifice.

\subsubsection{Comparing the horizontal and vertical sigmas}

The standard deviations of horizontal and vertical wind can be used to feed atmospheric dispersion models, and the question arises of ``which'' standard deviations to use.

In modeling applications it is customary to use the vertical and cross-wind variances, and this suggests \emph{rotated} covariances are of interest. One practical question then is, whether non-rotated variances can be used instead. A positive answer makes sense, as computing non-rotated variances is of course simpler than with rotated variances.

On the other side, this positive answer is not physically impossible to achieve. In fact, the sum $\sigma_{u}^{2} + \sigma_{v}^{2} + \sigma_{w}^{2}$ (incidentally, twice the turbulent kinetic energy) is the \emph{trace} of the covariance matrix, and as such is invariant with respect to transformations like

\begin{equation}\label{eq:Matrix Conjugation}
    \mathbf{S} \mathbf{C} \mathbf{S}^{T}
\end{equation}

\noindent where $\mathbf{S}$ is non-singular. Rotation of reference, as performed in classical eddy covariance or planar fit, is a special case of this same transform, so we may expect the sum of the three wind component variances to stay unchanged under rotation. On reasonably not-too-complex terrain we may expect that the vertical component will not change too much, so the same must also be true for the sum of the two \emph{horizontal} variances.

That this desirable fact really happens is site-dependent, and connected to the average absolute magnitude of the second rotation angle. The verification is simple however.

In figure \ref{fig:Sigma_uv} we see the effect of reference rotation in CRA.01 set, by applying classical eddy covariance with 2 rotations.

\begin{figure}[htp]
 \centering
 \begin{center}
 \includegraphics[scale=1.1,keepaspectratio=true]{./diagrams/sigma_uv.pdf}
 % get.raw.data.1.pdf: 288x180 pixel, 72dpi, 10.16x6.35 cm, bb=0 0 288 180
 \end{center}
 \caption{Horizontal standard deviation $\sqrt{\sigma_{u}^{2} + \sigma_{v}^{2}}$ in non rotated vs rotated frame (data set: CRA.01)}
 \label{fig:Sigma_uv}
\end{figure}

As we can see, rotated and non rotated horizontal variances are practically the same in this case. By the rotation invariance of turbulent kinetic energy the same must be for vertical standard deviation. But skepticism is a virtue in science, so let's check directly in figure \ref{fig:Sigma_w}.

\begin{figure}[htp]
 \centering
 \begin{center}
 \includegraphics[scale=1.1,keepaspectratio=true]{./diagrams/sigma_w.pdf}
 % get.raw.data.1.pdf: 288x180 pixel, 72dpi, 10.16x6.35 cm, bb=0 0 288 180
 \end{center}
 \caption{Vertical standard deviation $\sigma_{w}^{2}$ in non rotated vs rotated frame (data set: CRA.01)}
 \label{fig:Sigma_w}
\end{figure}

The similarity of rotated and non-rotated variances is not the full story. Another interesting question is, whether some relation exists between horizontal and vertical component variance. In some dispersion models a linear relation between horizontal and vertical variances is assumed.

\begin{figure}[htp]
 \centering
 \begin{center}
 \includegraphics[scale=1.1,keepaspectratio=true]{./diagrams/sigma_uv_vs_w.pdf}
 % get.raw.data.1.pdf: 288x180 pixel, 72dpi, 10.16x6.35 cm, bb=0 0 288 180
 \end{center}
 \caption{Horizontal and vertical standard deviations compared (data set: CRA.01)}
 \label{fig:Sigma_uv_vs_w}
\end{figure}

The scattering we can see in figure \ref{fig:Sigma_uv_vs_w} is visually significant, yet the zero-passing linear model with its 0.1431 horizontal-to-vertical sigma suggests a much stronger horizontal standard deviation.

The existence of a clean relation between vertical and horizontal sigma allows site specific parameterizations of the sigma, which may be inserted in an existing atmospheric dispersion model, possibly after data have been classified in stable and convective; better even, measured standard deviations may be used directly in the model.

\subsection{A caveat: intrinsic limitations of eddy covariance}

Eddy covariance is a very powerful technique, and as such it is often used in many applications of immediate value, as for example quantifying fluxes of water or nutrients at the interface between an ecosystem and the Earth atmosphere.

In so doing many implicit assumptions may be taken for true, and a verification is often worth.

One assumption is terrain and canopy to be flat and uniform, and it often is self-evident whether this condition is met by a field experiment. Other assumptions may be much less intuitive, and related to the complex physics involved.

Experience shows eddy covariance exhibits a tendency to underestimate fluxes of carbon dioxide on night-time, under stable conditions (see e.g. \cite{Aubinet2012}). Some recent work (see in particular \cite{Aubinet2010}) highlight the possibility that divergence of horizontal flux contribute significantly. This is unfortunate, as the measurements needed are expensive and difficult (more towers, equipped with sonics and gas sensors, operating as in sync as possible). And ``unfortunately'' this effect may not explain all, or most, of the vertical exchanges of carbon dioxide, other factors (as flux from ground) having a very important impact on observed data.

Things may be even tougher than said. In cases where the ``forgotten terms'' are negligible other factors may also occur inducing eddy covariance to underestimate fluxes.

An interesting possible effect has been identified on rough terrain (see \cite{Sakai2001}), where low-frequency eddies may give an important contribution to fluxes. Long-frequency eddies might easily be washed out as ``trend'' by standard eddy covariance performed aggressively enough.

Possible solutions may come from including the non-intuitive aspects in models, and using them to validate measurements or devise new experimental methodologies. Two examples of this approach are shown in \cite{Canepa2010} and \cite{Montagnani2010}.

Anyway an appreciation of which is the (scientific and practical) context of measurements is fundamental to gain knowledge from measurements, and not just huge piles of data.

%*************************************************************************

\section{Ultrasonic anemometry for wind power plants}
\subsection{Introduction}

Wind power plant economic feasibility at a given site depends critically on whether enough wind is available, and if this wind can be used without harm (that is, at acceptable maintenance cost).

Traditionally the problem of \emph{plant siting} is addressed in stages, beginning with a CFD simulation performing the screening of comparativelky large regions (some kilometers) to locate the most promising sites for turbines. Once these prospective sites are defined, measurements are taken for limited periods (typically two months) to confirm the CFD predictions.

Measurements can be taken in many ways. Perhaps the most common (and supported by current technical standards) is by using mechanical anemometers installed at different heights above ground on a meteorological tower. Technical alternatives exist, and are gaining momentum: for example, using small SODAR system for remote sensing wind speed at many altitudes. And, of course, ultrasonic anemometers. This is the application we'll cover in this section.

Ultrasonic anemometers promise many improvements over mechanical anemometers. Two of these may be of special concerns to wind power plant designers: the ability of ultrasonic anemometers to see also the vertical wind (to which conventional anemometers are blind) and being able to measure turbulence directly. Both turbulence and vertical wind affect negatively the long term reliability of turbines. Said differently: they are connected to financial losses.

Just as a remind as we are dealing with money issues: large turbines cost a monstruous lot. I've heard orders of magnitude of 2 millions Euros per 1.6 MW (to date a medium size turbine, tending towards the small side). Such big investments are typically acted by banks, who are willing to give their money if and only if they foresee solid prospectives of yield.

This means banks wish to predict the future financial yield (or loss) associated to a prospective wind power plant the most accurately they can. As this is not always possible (banks often lack the accurate knowledge of wind) they tend to rely on the predictions made by designers, by ensuring these people follow sound modeling and measurement practice.

How do they do? The ``usual'' way in engineering: by verifying the plant designers and proposers have followed a rigid protocol. The protocol is quite detailed, and reflects the existing technical standards and the measurement technology available when they were first formulated. Currently, banks ask for ``certified'' measurements - and for measurements to be certified they must have been taken using ``certified instruments'' (to date, some mechanical anemometers). SODARs and ultrasonic anemometers are superior, and provide information banks would like very much to have, but are often not ``certified''. Or if they are, certificates are valid only in some countries and their automatic acceptance in other countries is not granted (neither impossible, however: as far as I know, for example, certificates emitted in Germany are accepted without discussion in Italy). To specify in precise legal terms what a ``certificate'' is lays much beyond the purpose of this manual, and we'll not pursue it any longer.

Rather, we focus on what ultrasonic anemometers may yield of interest to bankers, plant designers and other stakeholders, that conventional anemometers can't provide. Maybe will this contribute a little bit in improving the obsolete technical standards in current use?

\subsection{The function ''wind.power''}

Function \verb|wind.power| yields wind power and maintenance related data, both as sets of time-stamped vectors and overall indicators. Analyzed by skilled people, these numbers give detailed information about wind from a power production standpoint, to a level much refined than using mechanical instruments.

Before to proceed with the description, an important detail is worth mentioning: focus is on \emph{wind}, not turbines. Any specific turbine has its own power curve, which may be vastly different than power curves for other models. Electing one turbine as reference would be unfair to all others, so we'll not do. On the other side: is knowledge of wind is detailed enough, then computing a yield prediction is quite simple if the turbine power curve is known.

To invoke the function you may type this command:


\subsection{Preparing data for ``wind.power''}

We mentioned banks and bankers, and the standards they enforce as a condition to supply money. One standard does directly apply to our case: time averaging. Which in wind power community is almost universally accepted to be 10 minutes.


%*************************************************************************

\chapter{Visualizing Data and Their Structure}
\section{Plotting data the simple way}

One important visualization of data, and maybe one of the most common, is just plotting them as functions of time.

SonicLib does not provide a function to perform this task, the almost-almighty \verb|plot| command of base R being orders of magnitude more than plain sufficiency. Here we give some idea, however, by showing some practical interactive examples.

If you are not familiar with R reading this section may prove worth the bit effort, both for its own utility and the possibility to extend knowledge gained here to customize the SonicLib diagrams presented in later sections.

As mentioned shortly, the plotting command used in R is

\begin{verbatim}
plot(x, y, ...)
\end{verbatim}

\noindent where \verb|x| and \verb|y| designate the two vectors (of the same length) containing the $x$ and $y$ coordinates on the plotting plane. Ellipsis are a placeholder indicating the (possibly long) set of parameters.

Parameters are of course of paramount importance, as their sensitive use allows to produce publication-quality plots with little effort. They exist in an impressive number, and if you like to browse among them we kindly invite you to have a look at R help (just as a starter, you may begin by giving command \verb|?plot| from within an R session; as your needs become more advanced you may want to consult specialized texts like \cite{Murrell2006}).

%*************************************************************************
\subsection{Example: Understanding non-stationarity with time domain plots}

As a first practical example suppose we investigate the reason of why relative non-stationarity of $w$ peaked at 15:30 in CRA.01 data set, but not at 15:00. As we've discovered in section \ref{sec:Sane skepticism} data set CRA.01 is characterized by a ``normal'' value of relative non stationarity on $w$, from which a few distinct peaks emerge. The one occurring at 15:30 characterizes for its narrow span, suggesting an unusual event suddenly coming in.

SonicLib raw data are stored in hourly file, whose naming convention (explained in \ref{sec:FileFormat}) allows to predict the one we're interested in is \verb|20100516.15.csv|: our first task is grabbing it into the R session:

\begin{verbatim}
f <- get.raw.data("../TestData/CRA.01/20100516.15.csv", verbose=TRUE)
\end{verbatim}

Having specified \verb|verbose=TRUE| allows to check for any error message. The function completed printing no message to session, so we can deduce all went right as far as SonicLib is concerned. But the final proof is in the result itself, which we may check giving the command

\begin{verbatim}
str(f)
\end{verbatim}

This done, the system answers presenting a short sample of the data stored into file:

\begin{verbatim}
List of 2
 $ data         :'data.frame':	35990 obs. of 7 variables:
  ..$ time.stamp: num [1:35990] 0 0 0 0 ...
  ..$ u         : num [1:35990] -0.27 -0.61 ...
  ..$ v         : num [1:35990] -2.18 -1.89 ...
  ..$ w         : num [1:35990] 0.28 0.06 ...
  ..$ t         : num [1:35990] 15.4 15.3 ...
  ..$ q         : num [1:35990] 273 277 ...
  ..$ c         : num [1:35990] 16.2 16.2 ...
 $ sampling.rate: num 10
 - attr(*, "class")= chr "sonic.raw.data"
>
\end{verbatim}

The summary given by \verb|str()| is interesting and infrmative: it allows to get an impression of the nature of data, and a bit of intuition will easily arrive to figure out the structure. R allows other summaries however: for example, \verb|summary()| applied to a data frame returns a complete statistic report of each column.

Among all other useful things, command \verb|str()| allows us to understand where the data we're interested in may be found. We may decide to store this knowledge in our neurons for later use, or make use of some other commands to make our life less painful now. We follow this second path, by isolating time stamp and vertical wind component in variables whose names is easy to remind and shorter to type:

\begin{verbatim}
t.s <- f$data$time.stamp
w <- f$data$w
\end{verbatim}

We may now engage the real problem, visualizing data. We may begin with the simplest possible plot:

\begin{verbatim}
plot(t.s,w)
\end{verbatim}

Result is shown in figure \ref{fig:Simple plot 0}. As you can see, the result is absolutely unsatisfactory! In fact, if no specific option is given the R default version of \verb|plot| just builds a scatter plot, with open circles at every data point. The circles are wide, and data plots in large number: the final result is filling a third of the plot with a black band, losing completely any hope to follow the actual variation.

\begin{figure}[htp]
 \centering
 \begin{center}
 \includegraphics[scale=1.1,keepaspectratio=true]{./diagrams/SimplePlot0.pdf}
 % get.raw.data.1.pdf: 288x180 pixel, 72dpi, 10.16x6.35 cm, bb=0 0 288 180
 \end{center}
 \caption{Variation of vertical wind speed with time, from 15:00 to 16:00 in CRA.01 set}
 \label{fig:Simple plot 0}
\end{figure}

What we really desire in fact is a \emph{line plot}. In R this means to specify an additional parameter:

\begin{verbatim}
plot(t.s, w, type="l")
\end{verbatim}

The new result is shown in figure \ref{fig:Simple plot 1}.

\begin{figure}[htp]
 \centering
 \begin{center}
 \includegraphics[scale=1.1,keepaspectratio=true]{./diagrams/SimplePlot1.pdf}
 % get.raw.data.1.pdf: 288x180 pixel, 72dpi, 10.16x6.35 cm, bb=0 0 288 180
 \end{center}
 \caption{Variation of vertical wind speed with time, from 15:00 to 16:00 in CRA.01 set}
 \label{fig:Simple plot 1}
\end{figure}

The result of our second attempt is a bit better than before: we now have a better impression of actual data variation. Yet the plot is still unsatisfactory: to begin with, axes descriptions are at best uninformative to people not already knowing the facts.

A better approach would be to indicate clearly what the variables are, and which measurement units have been used. This is simple for the vertical axis. But what of the horizontal? In this case the figure caption says the plot refers to an entire hour, and the range suggests seconds are used (this is consistent with the time stamp definition within SonicLib raw data files once read in memory). Seconds demands a lot of space to be printed: minutes may be preferable? And what of axis name? To tell, or to remove?

We try by improving the vertical axis, and removing axis name from the horizontal. We also convert to minutes:

\begin{verbatim}
plot(t.s/60, w, type="l", ylab="w (m/s)", xlab="")
\end{verbatim}

Here the result:

\begin{figure}[htp]
 \centering
 \begin{center}
 \includegraphics[scale=1.1,keepaspectratio=true]{./diagrams/SimplePlot2.pdf}
 % get.raw.data.1.pdf: 288x180 pixel, 72dpi, 10.16x6.35 cm, bb=0 0 288 180
 \end{center}
 \caption{Variation of vertical wind speed with time, from 15:00 to 16:00 in CRA.01 set}
 \label{fig:Simple plot 2}
\end{figure}

Figure \ref{fig:Simple plot 2}, too, is far from ideal, yet much better than \ref{fig:Simple plot 0}. We might now try to place ticks not where R likes, but where our aesthetical sense could suggest. Or, me may adopt a lighter color for $w$.

All these improvements can be made adding parameters. We might go on, refining our plot further, but we imagine you got the message. Any details may be found in \cite{RManual}.

But, we notice an important thing. Our last graph is finally \emph{useful}: it shows us a large change occurred in wind \emph{variation} (a second order like statistics). This variation occurs exactly where we expected it, in the second half-hour. Last but not least, if placed in a presentation would make immediately the point even to a casual listener.

Before closing this example we love to highlight an important concept:

Plotting data involves of course learning R powerful graphics commands, but its real purpose is \emph{telling a story}, and making a point. No story, poor graph. Before delving into the development of the fanciest plot it is advisable to think first what we would like to transmit.

This in turn demands a clear understanding of whom our story is aimed at. Stated differently, writing programs or using a top-notch software package is at least 90\% human relation and communication, and 10\% technology (and not be impressed by the commonly observed fact that industry rewards technology 90\% and good communication 10\%: it says a lot about the past, present and, likely, future ``financial crises'': Buggy by Design?)

Sure this manual has not space enough to get deep on this essential point, but fortunately there are many excellent references. Among them the authors have used (and found subjectively useful) \cite{Maindonald2007} and \cite{Venables2002}.

We proceed now to check where the anomalous relative non-stationarity comes from. According to its definition, the relative non-stationarity is proportional to the difference between minimum and maximum value of signal regression line. As the regression line is so important, it would be nice showing it.

The first step to get distinct regression lines is partitioning data with respect to time in first and second half hour. We may accomplish this result, by using the powerful indexing capabilities of R on the time stamp:

\begin{verbatim}
t.lower <- t.s <= 1800
t.upper <- t.s >  1800
\end{verbatim}

\verb|t.lower| and \verb|t.upper|, generated this way, are two boolean vectors of same length as \verb|w|. These vectors can be used to select the upper and lower part of vertical wind speed:

\begin{verbatim}
w.lower <- w[t.lower]
w.upper <- w[t.upper]
\end{verbatim}

We used time stamps as a selection tool, but it is not wise to employ them as independent variable in linear modeling. This, because in SonicLib raw data files time stamps are allowed to be integer (they are, in CRA.01 set) so repeated stamps are possible. An alternative is using positional indices, and we'll do this method. Partitioning indices is the same as with $w$; in addition we scale them so they can function as replacement of minutes.

\begin{verbatim}
idx <- 1:length(w)
idx.lower <- idx[t.lower]/600
idx.upper <- idx[t.upper]/600
\end{verbatim}

The next step is computing the models themselves. This is done using command \verb|lm()|, using partitioned indexes as independent variables, and partitioned $w$ as dependent:

\begin{verbatim}
l.lower <- lm(w.lower~idx.lower)
l.upper <- lm(w.upper~idx.upper)
\end{verbatim}

The result we obtained can then be inspected by giving commands \verb|l.lower| and \verb|l.upper| respectively. This is what we get with the lower half hour:

\begin{verbatim}
Call:
lm(formula = w.lower ~ idx.lower)

Coefficients:
(Intercept)    idx.lower  
 -0.0422577    0.0004308
\end{verbatim}

\noindent And this is the upper:

\begin{verbatim}
Call:
lm(formula = w.upper ~ idx.upper)

Coefficients:
(Intercept)    idx.upper  
   0.059042    -0.001693
\end{verbatim}

Note slope and intercept have opposite signs in the two cases. But values are small enough to make very difficult to predict visually what happens. We wish to display data and the regression lines, and our first attempt is by applying the command \verb|abline()|, applied to the regression models after having plotted the actual data:

\begin{verbatim}
plot(idx/600, w, type="l", ylab="w (m/s)", xlab="")
abline(l.lower, col="red")
abline(l.upper, col="blue")
\end{verbatim}

\noindent The result is shown in figure \ref{fig:Simple plot 3}.

\begin{figure}[htp]
 \centering
 \begin{center}
 \includegraphics[scale=1.1,keepaspectratio=true]{./diagrams/SimplePlot3.pdf}
 % get.raw.data.1.pdf: 288x180 pixel, 72dpi, 10.16x6.35 cm, bb=0 0 288 180
 \end{center}
 \caption{Variation of vertical wind speed with time, from 15:00 to 16:00 in CRA.01 set. In red the regression line computed on first half hour; in blue the regression line of second half hour.}
 \label{fig:Simple plot 3}
\end{figure}

This is not the best we may do: according to the way \verb|abline()| operates on regression lines, the whole span of independent variable is used, and this gives a really misleading view: it would be much better to plot the two lines only where they are defined. To do this, we could evaluate the regression models in correspondence of their definition intervals, then plot them.

The first step, evaluating the models in correspondence of the independent variable values, may be done the straightforward way once we know the regression coefficients:

\begin{verbatim}
m.lower <- l.lower$coefficients
m.upper <- l.lower$coefficients
\end{verbatim}

Proceeding this way instead of copying their values shown previously we get coefficients in full precision. Because R is done, \verb|m.lower[1]| represents the intercept and \verb|m.lower[2]| the slope of regression model \verb|l.lower| (and identically with \verb|l.upper|).This allows to perform our estimation:

\begin{verbatim}
w.est.lower <- m.lower[1] + idx.lower*m.lower[2]
w.est.upper <- m.upper[1] + idx.upper*m.upper[2]
\end{verbatim}

\noindent We are now ready to plot:

\begin{verbatim}
plot(idx/600, w, type="l", ylab="w (m/s)", xlab="")
lines(idx.lower,w.est.lower,col="red",lwd=3)
lines(idx.upper,w.est.upper,col="blue",lwd=3)
\end{verbatim}

The command \verb|lines()| plots a line graph without removing the underlying graphics, as \verb|plot()| would do by default. Parameter \verb|lwd| specifies the line width, with a minimum value of 1; value 3 is a sensible choice to make the line visible. The result is \ref{fig:Simple plot 4}.

\begin{figure}[htp]
 \centering
 \begin{center}
 \includegraphics[scale=1.1,keepaspectratio=true]{./diagrams/SimplePlot4.pdf}
 % get.raw.data.1.pdf: 288x180 pixel, 72dpi, 10.16x6.35 cm, bb=0 0 288 180
 \end{center}
 \caption{Variation of vertical wind speed with time, from 15:00 to 16:00 in CRA.01 set. In red the regression line computed on first half hour; in blue the regression line of second half hour.}
 \label{fig:Simple plot 4}
\end{figure}

The two regression lines are different, as we had seed when printing the model numerical values. But visually, their dissimilarity seems not striking.

%*************************************************************************
\subsection{Visualizing directional dependencies using R as it is}

One common reason of non-uniformity is connected to the different ``history'' experienced by wind as it flows through different environments. An indicator of this history is wind provenance direction.

One way to display the dependence of a variable on wind direction is a scatter plot with the wind direction used as an independent variable. But such a diagram is quite uninformative if we employ a scatter plot of the usual kind, like the one shown in figure \ref{fig:Normal directional scatter plot}, showing the change of the ratio of friction velocity to the wind speed with respect of a ``long'' data set.

\begin{figure}[htp]
 \centering
 \begin{center}
 \includegraphics[scale=1,keepaspectratio=true]{./diagrams/UstarOverVel_vs_Dir_Standard.pdf}
 % get.raw.data.1.pdf: 288x180 pixel, 72dpi, 10.16x6.35 cm, bb=0 0 288 180
 \end{center}
 \caption{Directional scatter plot of $u_{*}/U$ drawn using the standard ``plot'' command}
 \label{fig:Normal directional scatter plot}
\end{figure}

The reason figure \ref{fig:Normal directional scatter plot} is not the best is the heavy clutter of circles: if data are too much a significant danger exists they overlap in a way not allowing to appreciate where they're most concentrated.

An alternative in this case is using command \verb|smoothScatter|, now part of the standard R distribution. The syntax is identical to \verb|plot|'s, but instead of small circles in correspondence of data points, pixel in shades of gray are drawn proportional to the number of data points in the nearby. In practice, the higher the concentration, the darker the tone. As we may see in figure \ref{fig:Improved directional scatter plot}.

\begin{figure}[htp]
 \centering
 \begin{center}
 \includegraphics[scale=1,keepaspectratio=true]{./diagrams/UstarOverVel_vs_Dir.pdf}
 % get.raw.data.1.pdf: 288x180 pixel, 72dpi, 10.16x6.35 cm, bb=0 0 288 180
 \end{center}
 \caption{Directional scatter plot of $u_{*}/U$ drawn using command ``smoothScatter''}
 \label{fig:Improved directional scatter plot}
\end{figure}

What happens around 130 degrees? As we may see, the ratio of friction velocity to wind speed is close to a fashionable 10\% in all directions, with the exception of the sector 100 to 170 degrees, where we can see a sharp rise to a 30\%, followed by a complementary sharp decrease.

Visual inspection of aerial photographs of field site suggests a possible reason: a tall and wide tree standing to the SE of measurement point, at around 25 meters distance. In theory, fixed stations should be placed at least at a distance ten times the height of major obstructions at any possible direction. But this requirement is really difficult to fulfil in practice, especially in densely populated areas. And more so at \emph{urban} sites. 

If this is the case, a directional diagram of some key turbulence indicators may work well as a data quality assessment tool. It may also be helpful in quality \emph{assurance}, for example by labeling data associated to wind direction from some critical directions. If ``all'' directions are critical, then a convincing argument for relocating an existing station is also easy to build.

\section{SonicLib-specific Plots and Related Functions}

\subsection{Function dir.mean}
\subsubsection{Function use}

To use \verb|dir.mean| the following command should be issued:

\begin{verbatim}
dir.mean(
  dir,
  val,
  width,
  step,
  confidence,
  boot.size,
  conf.level
)
\end{verbatim} 

\noindent where

\begin{description}
 \item[dir] Vector containing direction values for each data set elements (degrees from North; no default).
 \item[val] Vector, having the same dize as ``dir'', and containing all values for each data set element (unit depends on actual value meanings; no default).
 \item[width] Sector width (degrees; default = 11.25).
 \item[step] Angle between the center directions of two consecutive steps (degrees; default = 1).
 \item[confidence] Method to be used when computing confidence intervals (alphanumeric string, with possible values ``none'' and ``boot''; default: ``boot'').
 \item[boot.size] Sample size to be used when computing confidence limits using bootstrap method (positive integer number; default = 1000; this value is ignored if ``confidence'' is not ``boot'').
 \item[conf.level] Value of the confidence level at which confidence intervals are computed (default = 0.95).
\end{description}

The result is an instance of an object \verb|dir.stat| in case of successful completion, or NULL otherwise.

\subsubsection{Separate or overlapping sectors?}

If the function defaults are used, sectors are guaranteed to overlap widely as each of them is 11.25 degrees wide (half  the familiar 16-wind-sectors rule), and the sector-to-sector angular spacing is just 1 degree.

This way, passing from a sector to the next many data values get the chance of being counted twice. Is this a big deal? In many cases ``no'' could be an appropriate answer: an indicative mean is just good.

This needs not be the universal case however. Proper selection of \verb|width| and \verb|step| allows customization. A common non-default choice consists in selecting 16 sectors, with \verb|width=22.5| and \verb|step=22.5|, in which case sectors overlap only on their boundary (a line, with zero area), behaving as practically separate.

\subsubsection{Confidence intervals}

Function \verb|dir.mean| provides parameters allowing to estimate confidence intervals on each directional mean. The meaning of confidence interval is not to give an idea of dispersion of values around the mean (although it could be mathematically related to it), but rather to specify an interval such that the true directional mean belongs to it at a specified confidence level.

The true and computed directional means should in fact not be confused, the latter being an estimate of the former, whose actual value is not a priori known.

Confidence limits can be used as a visual tool (they will, using directional mean plotting functions within SonicLib), but may also be used numerically to solve common problems on statistically firmer ground than just proceding intuitively. Suppose for example you are interested in checking the value of friction velocity seen by wind coming from a given direction in, say, winter and summer - possibly to check for changes in turbulence generation by deciduous shrubs upstream.

Without confidence limits you'd presumably get the directional means computed in the two periods, compare them, and likely decide they are somewhat different. This is not a strong conclusion, indeed: the two numerical values of means along the same direction may not be equal, but this do not prove beyond any doubt their true values to be also different.

If we have confidence interval at hand, solving the problem in more meaningful terms is easier: it suffices to see whether the two confidence intervals have void intersection or not. In the first case you can be ``reasonably sure'' (within confidence level) the true directional means are different. In the second, you can't exclude their equality. Even if the overlap zone is small compared to interval overall size.

Similar considerations can be made for other problems of practical interest. For example: is temperature changing with wind direction, or not? Overall, confidence limits are a precious indication.

\subsubsection{Examples}

Using function \verb|dir.mean| is straightforward: all you need are two equal size vectors holding wind direction and the quantity you want to average directionally. Data may actually be raw sonic wind vectors, averaged sonic data, eddy covariance results, chemical concentrations from given directions. Literally, whatever (in fact \verb|dir.mean| could be used for applications having nothing in common with sonic anemometry).

Here is a very simple example from eddy covariance results:

\begin{verbatim}
e.dm<-dir.mean(e$data$Dir, e$data$u.star)
\end{verbatim}

Once you have computed the directional mean you may do interesting things jut using numerical values. Here for example we check whether friction velocity is uniform with respect to wind directions or not, using Vertemate data set.

Of course, if we reason the simple way the answer will almost invariably be ``friction velocity is not uniform with respect to wind direction''. Indeed, the value of directional means will show some degree of change as we may see easily:

\begin{verbatim}
> min(e.dm$mean, na.rm=TRUE)
[1] 0.134949
> max(e.dm$mean, na.rm=TRUE)
[1] 0.4044134
\end{verbatim}

The maximum directional mean found is three times the minimum! Little doubt they look ``different''. But this is true of the means computed from experimental data, that is, \emph{estimations} of the true means. What about the latter?

To answer we need some more information: first of all, a more precise definition of what we mean by ``not uniform with respect to direction''. Second, but not less important, we need knowing where we may find the true value, and here is where confidence limits come in.

We begin with the definition of (non-)uniformity. We have many directional mean values, each computed from their equally-spaced sectors. On each sector, corresponding to its direction $\delta$, we also have confidence limits, say them $l(\delta)$ and $u(\delta)$ with $l(\delta) \le u(\delta)$.

Now get two distinct directions, $\delta_{1}$ and $\delta_{2}$. In case of non-uniformity their intervals have an empty intersection. Closed intervals on the real time are special cases of closed sets. The complement of a closed set is an open set. The complement of the intersection of two set is the union of their complements, which are both open. Now, the union of two open sets is also open. This might not seem on the first instance, but this is a very important conclusion: it implies the two non-intersecting closed sets are separated by an open sets which, by its very nature, is fat enough to contain quite huge subsets (closed included).

In terms of intervals, this means that either $u(\delta_{1}) < l(\delta_{2})$ or $u(\delta_{2}) < l(\delta_{1})$.

But: we have many confidence intervals, one per direction value, not just two. Saying a quantity is non-uniform could reasonably mean the confidence intervals of at least two directions do not overlap. In case of our set:


\begin{verbatim}
> max(e.dm$conf.min, na.rm=TRUE)
[1] 0.372677
> min(e.dm$conf.max, na.rm=TRUE)
[1] 0.1478555
\end{verbatim}

The minimum of confidence intervals upper limits is smaller than the maximum of lower confidence limits, so at least two intervals do not overlap.


\subsection{Function plot.dir.mean}
\subsubsection{Function use}

To invoke \verb|plot.dir.mean| a command like this is to be used:

\begin{verbatim}
plot.dir.mean(
  d.m,
  min.val,
  max.val,
  col,
  add,
  conf.limits,
  verbose
)
\end{verbatim}

\noindent where

\begin{description}
 \item[d.m] is an instance of \verb|dir.stat| type, described in section \ref{sec:dir.stat}, and returned by function \verb|dir.mean|.
 \item[min.val] Minimum value to be used in plotting (no default).
 \item[max.val] Maximum value to be used in plotting (no default).
 \item[col] Color (alphanumeric string, defaulting to ``black'').
 \item[add] Boolean, stating whether current plot is to be added (TRUE) to a currently existing directional mean plot, or not (FALSE) (default: FALSE).
 \item[conf.limits] Boolean, specifying whether confidence limits should be plotted or not.
 \item[verbose] Boolean, specifying whether error message are desired (TRUE) or not (FALSE) (default: FALSE).
\end{description}

\subsubsection{How to select minimum and maximum values}

Two of the function parameters specify the minimum and maximum values to be used as radial limits during plotting. Only the part of diagram within these limits are actually drawn, so a sensible selection is of paramount importance.

Directional mean plots are no different from other types of statistical plots. They adopt polar form, but selecting in different way the limits allows to present the same data letting people understand quite different things. This is the reason minimum and maximum values have no default: they are too important to ask SonicLib doing an arbitrary choice.

But this choice conduce to the need of discovering an interval guaranteeing the plot to be entirely visible. The conduct is different depending on whether confidence limits are to be plotted or not. In the first case, the minimum spanning interval is obtained using commands

\begin{verbatim}
min.val <- min(d.m$c95.inf)
max.val <- max(d.m$c95.sup)
\end{verbatim}

\noindent where \verb|d.m| denotes the directional mean object returned by function \verb|dir.mean|. If confidence intervals are not interesting, the commands reduce to

\begin{verbatim}
min.val <- min(d.m$mean)
max.val <- max(d.m$mean)
\end{verbatim}

The mere fact an interval allowing to plot the whole directional mean does not imply the corresponding graph makes actual sense. But at least provides us with a good starting point. Producing good graph is both a matter of sound science and good communication, and many guidelines can be found easily by surfing on the web or consulting any text of applied statistics. So we'll not pursue this important detail any more.


\subsubsection{Examples}

As directional statistics are so important as practical tool, in SonicLib a few functions address them specifically (in particular \verb|dir.mean| to perform the actual computing and \verb|plot.dir.mean| for the actual plotting). Using SonicLib to plot directional means yields both aesthetical and practical advantages: on one side it represents directions in polar form, for a more intuitive feeling of data meanings; on the other it allows to visualize quantities helping to assess the quality of data, as for example confidence limits.

\begin{figure}[htp]
 \centering
 \begin{center}
 \includegraphics[scale=1,keepaspectratio=true]{./diagrams/DirMeanUstar_1.pdf}
 % get.raw.data.1.pdf: 288x180 pixel, 72dpi, 10.16x6.35 cm, bb=0 0 288 180
 \end{center}
 \caption{Directional plot of $u_{*}$ drawn using command ``plot.dir.mean'' without confidence limits}
 \label{fig:Directional plot 1}
\end{figure}

Figure \ref{fig:Directional plot 1} shows the simplest possible graph of directional mean of friction velocity $u_{*}$ plotted using SonicLib functions. The data set has been prepared by averaging raw data files in standard SonicLib format, then running standard eddy covariance (with no frills) on the results. Finally, directional means have been computed along with confidence limits using \verb|dir.mean| function:

\begin{verbatim}
d<-average.sonic.file.set(file.name)
e<-eddy.covariance(d, 300)
e.dm<-dir.mean(e$data$Dir, e$data$u.star)
\end{verbatim}

\noindent And this is the actual plot command:

\begin{verbatim}
plot.dir.mean(e.dm, 0, 0.5)
\end{verbatim}

Plot \ref{fig:Directional plot 1} says not so much more than directional mean scatter plot as we've seen already. But does in a way allowing to better match, say, directions with a site map or photo. We may do more, however, for example to see how solid the directional mean is around the whole directions circle.

We can check this using confidence limits, as in figure \ref{fig:Directional plot 2}, obtained using a slightly revised form of the plot command:

\begin{verbatim}
plot.dir.mean(e.dm, 0, 0.5,
   conf.limits=TRUE)
\end{verbatim}

\begin{figure}[htp]
 \centering
 \begin{center}
 \includegraphics[scale=1,keepaspectratio=true]{./diagrams/DirMeanUstar_2.pdf}
 % get.raw.data.1.pdf: 288x180 pixel, 72dpi, 10.16x6.35 cm, bb=0 0 288 180
 \end{center}
 \caption{Directional plot of $u_{*}$ drawn using command ``plot.dir.mean'' with confidence limits}
 \label{fig:Directional plot 2}
\end{figure}


\subsection{Function plot.dir.num}
\subsubsection{Function use}

The width of confidence interval increases with decreasing sample size, so it is sometimes important to have an idea of how many data are present in each direction sector. This result is accomplished by a specific function:

\begin{verbatim}
plot.dir.num(d.m,col,add)
\end{verbatim}

\noindent where

\begin{description}
 \item[d.m] is an instance of \verb|dir.stat| type, described in section \ref{sec:dir.stat}, and returned by function \verb|dir.mean|.
 \item[col] Color (alphanumeric string, defaulting to ``black'').
 \item[add] Boolean, stating whether current plot is to be added (TRUE) to a currently existing directional mean plot, or not (FALSE) (default: FALSE).
\end{description}


\subsubsection{Examples}

We use the same friction velocity data seen in \verb|plot.dir.mean|, with results in figure \ref{fig:Numerosity plot}.

\begin{figure}[htp]
 \centering
 \begin{center}
 \includegraphics[scale=1,keepaspectratio=true]{./diagrams/DirNumUstar.pdf}
 % get.raw.data.1.pdf: 288x180 pixel, 72dpi, 10.16x6.35 cm, bb=0 0 288 180
 \end{center}
 \caption{Directional plot of data numerosity for $u_{*}$ set obtained using ``plot.dir.num''}
 \label{fig:Numerosity plot}
\end{figure}

Obtaining the graph has been very simple:

\begin{verbatim}
plot.dir.num(e.dm)
\end{verbatim}

\noindent where \verb|e.dm| is the same $u_{*}$ directional average set used in the description of \verb|plot.dir.mean|.

Despite being an extremely simple test, the numerosity plot often yields interesting information. In our case for example we may distinguish two very strong dominant directions, around 240 and 345 degrees. Other directions, for example from 45 degrees, are very much less represented.

Cases like this may happen if a very short period is considered, as in our test data set. Would a similar situation occur over a longer period (one year or more) the suspect would be some directions are severely screened by nearby obstructions. This indeed is not a rare finding, especially in urban and peri-urban situations.

%*************************************************************************
\section{Using statistics}

As a statistical system, R is well versed to answer questions in a statistically founded way. This may be an important advantage, in a field characterized by large sets of data sampled with high accuracy, but also by a large intrinsic variability.


%*************************************************************************
\subsubsection{Which direction is wind blowing from?}


%*************************************************************************
\subsubsection{Short-range pollutant dispersion}

One useful application of ultrasonic anemometers is performing high-accuracy measurements of mean wind and turbulent to feed atmospheric pollutant dispersion models. In this respect, besides seeing things normal anemometers do not sense, ultrasonic anemometers are also immune from ``wind calms'' and ``variable winds'', that is when wind speed is not fast enough to allow movement of the delicate mechanical parts constituting conventional anemometers. Immunity to wind calms and variable winds is very important in air quality studies because these condition correspond to the most dangerous situations, when transport by mean wind is minimum thus favoring the accumulation of large bulks of concentration.

Existing pollutant dispersion models are complex mathematical tools, whose description is beyond the scope of this work. What's important to our purposes is these models greatly benefit when fed with the least ``cooked'' averages possible. Basically, dispersion models may be given meteorological input ranging from a minimum, when wind speed and direction and air temperature are sufficient, to complex sets with turbulence data. Many models allow to specify various forms for meteorological input, all differing by the detail and quality of observation.

Surely the easiest way to use a model is feeding it with the less detailed meteorological data set possible. But in this case the model will derive all the turbulence data it needs for its internal working by \emph{estimates}, obtained applying various algorithms to the more or less scant input data. The less detailed the input meteo data, the more aggressive the estimation process. But also, the more assumption will be made behind the scene, and the larger the error will be. In extreme case, the model estimates are made using assumptions so far from reality that results are little more than fantasy work.

As an example, let's consider turbulence and the diffusion associated to it. A very popular way to express turbulence, at least until some years ago, was by using \emph{stability categories}. The introduction of stability categories to express turbulent diffusion quaitatively dates bach to the times of First World War, when empirical observation were made on field about the ``effectiveness'' of chemical aggressives depending on wheather conditions and time of day. Gradually various classifications emerged, not exactly comparable to each other, but all expressing the same basic concept.

Clearly stability categories are not a strictly quantitative data, and their use in models also gave rise to the production of output with a quantitative appearance (e.g. time series of ground concentration fields), but with a deeply qualitative nature which most practitioners (engineers with unsophisticate mathematical skills) did not appreciate.

Of course dispersion models can not use stability category directly. Rather, they translate them to ``plume dispersion indicators'' like the ``downwind horizontal and vertical standard deviations of concentration'' in Gaussian models. Horizontal and vertical standard deviations are functions of downwind distance from emission point, whose mathematical nature is usually quite simple (for example, in one of the most used scheme, due to Briggs, the plume dispersions are power laws whose coefficients change with stability category, and on whether a ``rural'' or ``urban'' case is in use).

Some models, more recent, allow to give turbulence information in quantitative form, but use a rather indirect approach. Instead of being satisfied of estimating the sigmas from the qualitative information stored in stability category, they estimate them from turbulence indicators like Obukhov length or friction velocity (both readily obtained from ultrasonic anemometer data using eddy covariance).

This is too an estimate. But is less aggressive and its output changes continuously with input values: we're in front to truly \emph{quantitative} data.

A further step is in using \emph{measured} sigmas directly, by relating the dispersion of a plume to staistical characteristics of wind as it is. Of course the sigmas of plume are not the same thing as the standard deviation of wind in the three directions, but a strong physical connection can be reasonably expected among them. 

The last possible step is, using ultrasonic anemometer data themselves, turbulent fluctuations included.

%*************************************************************************
\nocite{Arya2001}
\nocite{Aubinet2012}
\nocite{Foken2008}
\nocite{Kaimal1994}
\nocite{Leeder2011}
\nocite{Nobel2009}
\nocite{Oke1978}
\nocite{Sozzi2002}
\nocite{Stull1988}
\nocite{Valentini2002}
\nocite{VanDijk2004}
\nocite{Waring2007}
\nocite{RManual}
\nocite{Maindonald2007}
\nocite{Venables2002}
\nocite{Venables2000}
\bibliography{SonicLib}
\bibliographystyle{plain} 
%*************************************************************************

\end{document}
